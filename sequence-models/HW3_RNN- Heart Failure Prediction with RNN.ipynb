{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a28efa6caa45fb2f3142586b9c59adc",
     "grade": false,
     "grade_id": "cell-52506fc51faeb1a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW3 Recurent Neural Network\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this homework, you will build a bi-directional RNN on diagnosis codes. The recurrent nature of RNN allows us to model the temporal relation of different visits of a patient. More specifically, we will still perform **Heart Failure Prediction**, but with different input formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32e72084469253ba7b428e2d0bd46613",
     "grade": false,
     "grade_id": "cell-dcd6c662fba70926",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.621466Z",
     "start_time": "2021-12-10T02:49:07.849594Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2db74f9db6e96fc42296ed510b0f4be1",
     "grade": false,
     "grade_id": "cell-4fe346254a16fed8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# Define data path\n",
    "DATA_PATH = \"../HW3_RNN-lib/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "212e434fd38be5ca223e82a1e1fddf5b",
     "grade": false,
     "grade_id": "cell-71f2f1fcbf0214c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08063ba06edc72626d45a1f16564745e",
     "grade": false,
     "grade_id": "cell-f24c5a8a552afa64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## About Raw Data\n",
    "\n",
    "To get started, we will implement a naive RNN model for heart failure prediction using the diagnosis codes.\n",
    "\n",
    "We will use the same dataset synthesized from [MIMIC-III](https://mimic.physionet.org/gettingstarted/access/), but with different input formats.\n",
    "\n",
    "The data has been preprocessed for you. Let us load them and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.630910Z",
     "start_time": "2021-12-10T02:49:08.623252Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0dd1f4063e22be64f2709deffde7a7b",
     "grade": false,
     "grade_id": "cell-0d031c45ba4a787e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pids = pickle.load(open(os.path.join(DATA_PATH,'train/pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH,'train/vids.pkl'), 'rb'))\n",
    "hfs = pickle.load(open(os.path.join(DATA_PATH,'train/hfs.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH,'train/seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH,'train/types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH,'train/rtypes.pkl'), 'rb'))\n",
    "\n",
    "assert len(pids) == len(vids) == len(hfs) == len(seqs) == 1000\n",
    "assert len(types) == 619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1427cf82d51752cd4e90e7d483141ffe",
     "grade": false,
     "grade_id": "cell-66a0abe057d9ca85",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where\n",
    "\n",
    "- `pids`: contains the patient ids\n",
    "- `vids`: contains a list of visit ids for each patient\n",
    "- `hfs`: contains the heart failure label (0: normal, 1: heart failure) for each patient\n",
    "- `seqs`: contains a list of visit (in ICD9 codes) for each patient\n",
    "- `types`: contains the map from ICD9 codes to ICD-9 labels\n",
    "- `rtypes`: contains the map from ICD9 labels to ICD9 codes\n",
    "\n",
    "Let us take a patient as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.636763Z",
     "start_time": "2021-12-10T02:49:08.632459Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0d3847b04bd6dadf5ff567bee973690",
     "grade": false,
     "grade_id": "cell-ae331190a6d48106",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: 47537\n",
      "Heart Failure: 0\n",
      "# of visits: 2\n",
      "\t0-th visit id: 0\n",
      "\t0-th visit diagnosis labels: [12, 103, 262, 285, 290, 292, 359, 416, 39, 225, 275, 294, 326, 267, 93]\n",
      "\t0-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_518', 'DIAG_560', 'DIAG_567', 'DIAG_569', 'DIAG_707', 'DIAG_785', 'DIAG_155', 'DIAG_456', 'DIAG_537', 'DIAG_571', 'DIAG_608', 'DIAG_529', 'DIAG_263']\n",
      "\t1-th visit id: 1\n",
      "\t1-th visit diagnosis labels: [12, 103, 240, 262, 290, 292, 319, 359, 510, 513, 577, 307, 8, 280, 18, 131]\n",
      "\t1-th visit diagnosis codes: ['DIAG_041', 'DIAG_276', 'DIAG_482', 'DIAG_518', 'DIAG_567', 'DIAG_569', 'DIAG_599', 'DIAG_707', 'DIAG_995', 'DIAG_998', 'DIAG_V09', 'DIAG_584', 'DIAG_031', 'DIAG_553', 'DIAG_070', 'DIAG_305']\n"
     ]
    }
   ],
   "source": [
    "# take the 3rd patient as an example\n",
    "\n",
    "print(\"Patient ID:\", pids[3])\n",
    "print(\"Heart Failure:\", hfs[3])\n",
    "print(\"# of visits:\", len(vids[3]))\n",
    "for visit in range(len(vids[3])):\n",
    "    print(f\"\\t{visit}-th visit id:\", vids[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis labels:\", seqs[3][visit])\n",
    "    print(f\"\\t{visit}-th visit diagnosis codes:\", [rtypes[label] for label in seqs[3][visit]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pids ‚Üí list/array of patient IDs\n",
    "\n",
    "hfs ‚Üí list/array that says if a patient has heart failure (True/False or 1/0)\n",
    "\n",
    "vids ‚Üí list of lists ‚Üí each patient has multiple visit IDs\n",
    "\n",
    "seqs ‚Üí list of lists ‚Üí each patient has multiple visits, and each visit has diagnosis labels (numbers)\n",
    "\n",
    "rtypes ‚Üí dictionary that maps diagnosis labels (numbers) to their actual diagnosis codes/names\n",
    "\n",
    "Loop through each visit for this patient:\n",
    "\n",
    "vids[3][visit] = the visit‚Äôs ID\n",
    "\n",
    "seqs[3][visit] = the list of diagnosis labels for that visit (like [12, 45, 88])\n",
    "\n",
    "[rtypes[label] for label in seqs[3][visit]] = convert those labels into actual diagnosis codes (e.g., [\"I50.9\", \"E11.9\", \"J45.9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6484d87ff7b7dcc915b95cd6496a47f",
     "grade": false,
     "grade_id": "cell-945119717fb61cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note that `seqs` is a list of list of list. That is, `seqs[i][j][k]` gives you the k-th diagnosis codes for the j-th visit for the i-th patient.\n",
    "\n",
    "And you can look up the meaning of the ICD9 code online. For example, `DIAG_276` represetns *disorders of fluid electrolyte and acid-base balance*.\n",
    "\n",
    "Further, let see number of heart failure patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.642695Z",
     "start_time": "2021-12-10T02:49:08.639962Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d423d6893adfdc928488e362a86f3a",
     "grade": false,
     "grade_id": "cell-e6d339169f140694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of heart failure patients: 548\n",
      "ratio of heart failure patients: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"number of heart failure patients:\", sum(hfs))\n",
    "print(\"ratio of heart failure patients: %.2f\" % (sum(hfs) / len(hfs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-21T13:05:03.202250Z",
     "start_time": "2020-10-21T13:05:03.199011Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c20a3d6188476868bb0477e6493dcc29",
     "grade": false,
     "grade_id": "cell-0a48d6dcc0f5b4ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we have the data. Let us build the naive RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfdb69398ff06e79f0ec1d8faeeb4d5c",
     "grade": false,
     "grade_id": "cell-308c526175fdb62e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Build the dataset [30 points]\n",
    "\n",
    "### 1.1 CustomDataset [5 points]\n",
    "\n",
    "First, let us implement a custom dataset using PyTorch class `Dataset`, which will characterize the key features of the dataset we want to generate.\n",
    "\n",
    "We will use the sequences of diagnosis codes `seqs` as input and heart failure `hfs` as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.648717Z",
     "start_time": "2021-12-10T02:49:08.644248Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        # Just store references\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "        #raise NotImplementedError\n",
    "         # (Optional) sanity check\n",
    "        if len(self.x) != len(self.y):\n",
    "            raise ValueError(f\"Mismatched lengths: len(seqs)={len(self.x)} != len(hfs)={len(self.y)}\")\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.x)\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return \n",
    "        #raise NotImplementedError\n",
    "        self.x[index], self.y[index]\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.x = [[visit1, visit2], [visit1]]   # 2 patients\n",
    "self.y = [0, 1, 1]                      # 3 labels\n",
    "\n",
    "Here, inputs = 2 patients, labels = 3 ‚Üí mismatch ‚ö†Ô∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, seqs, hfs):\n",
    "        \"\"\"\n",
    "        Store the sequences and labels/targets without any permutation\n",
    "        and WITHOUT converting to tensors (conversion can be done later\n",
    "        in a collate_fn or inside the training loop).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seqs : Sequence\n",
    "            Input sequences/features (e.g., list of patient sequences).\n",
    "        hfs : Sequence\n",
    "            Targets/labels corresponding 1‚Äëto‚Äë1 with seqs.\n",
    "        \"\"\"\n",
    "        # Just store references\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "\n",
    "        # (Optional) sanity check\n",
    "        if len(self.x) != len(self.y):\n",
    "            raise ValueError(f\"Mismatched lengths: len(seqs)={len(self.x)} != len(hfs)={len(self.y)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples (patients).\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample by index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            (sequence, target) pair for the given index.\n",
    "        \"\"\"\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.658110Z",
     "start_time": "2021-12-10T02:49:08.650497Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9698147ad965e9a4336317e632e30259",
     "grade": true,
     "grade_id": "cell-cc0baa6c9dadef8c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "dataset = CustomDataset(seqs, hfs)\n",
    "\n",
    "assert len(dataset) == 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4ef302b3eea1353a0c52f3110cec061",
     "grade": false,
     "grade_id": "cell-de0d816943d88377",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Collate Function [20 points]\n",
    "\n",
    "As you note that, we do not convert the data to tensor in the built `CustomDataset`. Instead, we will do this using a collate function `collate_fn()`. \n",
    "\n",
    "This collate function `collate_fn()` will be called by `DataLoader` after fetching a list of samples using the indices from `CustomDataset` to collate the list of samples into batches.\n",
    "\n",
    "For example, assume the `DataLoader` gets a list of two samples.\n",
    "\n",
    "```\n",
    "[ [ [0, 1, 2], [8, 0] ], \n",
    "  [ [12, 13, 6, 7], [12], [23, 11] ] ]\n",
    "```\n",
    "\n",
    "where the first sample has two visits `[0, 1, 2]` and `[8, 0]` and the second sample has three visits `[12, 13, 6, 7]`, `[12]`, and `[23, 11]`.\n",
    "\n",
    "The collate function `collate_fn()` is supposed to pad them into the same shape (3, 4), where 3 is the maximum number of visits and 4 is the maximum number of diagnosis codes.\n",
    "\n",
    "``` \n",
    "[ [ [0, 1, 2, *0*], [8, 0, *0*, *0*], [*0*, *0*, *0*, *0*]  ], \n",
    "  [ [12, 13, 6, 7], [12, *0*, *0*, *0*], [23, 11, *0*, *0*] ] ]\n",
    "```\n",
    "\n",
    "Further, the padding information will be stored in a mask with the same shape, where 1 indicates that the diagnosis code at this position is from the original input, and 0 indicates that the diagnosis code at this position is the padded value.\n",
    "\n",
    "```\n",
    "[ [ [1, 1, 1, 0], [1, 1, 0, 0], [0, 0, 0, 0] ], \n",
    "  [ [1, 1, 1, 1], [1, 0, 0, 0], [1, 1, 0, 0] ] ]\n",
    "```\n",
    "\n",
    "Lastly, we will have another diagnosis sequence in reversed time. This will be used in our RNN model for masking. Note that we only flip the true visits.\n",
    "\n",
    "``` \n",
    "[ [ [8, 0, *0*, *0*], [0, 1, 2, *0*], [*0*, *0*, *0*, *0*]  ], \n",
    "  [ [23, 11, *0*, *0*], [12, *0*, *0*, *0*], [12, 13, 6, 7] ] ]\n",
    "```\n",
    "\n",
    "And a reversed mask as well.\n",
    "\n",
    "```\n",
    "[ [ [1, 1, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0] ], \n",
    "  [ [1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 1, 1], ] ]\n",
    "```\n",
    "\n",
    "We need to pad the sequences into the same length so that we can do batch training on GPU. And we also need this mask so that when training, we can ignored the padded value as they actually do not contain any information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let‚Äôs visualize it step by step with a simple diagram.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Example: 2 patients\n",
    "\n",
    "* Patient 1 has 2 visits\n",
    "* Patient 2 has 3 visits\n",
    "\n",
    "#### Raw data\n",
    "\n",
    "```\n",
    "Patient 1: [ [0, 1, 2], [8, 0] ]\n",
    "Patient 2: [ [12, 13, 6, 7], [12], [23, 11] ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Pad to same shape (3 visits √ó 4 codes)\n",
    "\n",
    "```\n",
    "Patient 1: [ [0,  1,  2,  0],\n",
    "             [8,  0,  0,  0],\n",
    "             [0,  0,  0,  0] ]\n",
    "\n",
    "Patient 2: [ [12, 13, 6,  7],\n",
    "             [12,  0, 0,  0],\n",
    "             [23, 11, 0,  0] ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Mask (1 = real value, 0 = padding)\n",
    "\n",
    "```\n",
    "Patient 1 mask: [ [1, 1, 1, 0],\n",
    "                  [1, 1, 0, 0],\n",
    "                  [0, 0, 0, 0] ]\n",
    "\n",
    "Patient 2 mask: [ [1, 1, 1, 1],\n",
    "                  [1, 0, 0, 0],\n",
    "                  [1, 1, 0, 0] ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Reverse visits (most recent first)\n",
    "\n",
    "```\n",
    "Patient 1 reversed: [ [8,  0, 0, 0],\n",
    "                      [0,  1, 2, 0],\n",
    "                      [0,  0, 0, 0] ]\n",
    "\n",
    "Patient 2 reversed: [ [23, 11, 0, 0],\n",
    "                      [12,  0, 0, 0],\n",
    "                      [12, 13, 6, 7] ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Reverse mask\n",
    "\n",
    "```\n",
    "Patient 1 reversed mask: [ [1, 1, 0, 0],\n",
    "                           [1, 1, 1, 0],\n",
    "                           [0, 0, 0, 0] ]\n",
    "\n",
    "Patient 2 reversed mask: [ [1, 1, 0, 0],\n",
    "                           [1, 0, 0, 0],\n",
    "                           [1, 1, 1, 1] ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚ú® **Big picture diagram**\n",
    "\n",
    "```\n",
    "Raw data  ‚Üí  Padded data  ‚Üí  Mask\n",
    "                    ‚Üì\n",
    "             Reversed data ‚Üí Reversed mask\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ Now, the model can process everything in neat, same-sized batches ‚Äî and thanks to the mask, it knows which numbers are ‚Äúreal‚Äù and which are just padding.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.667617Z",
     "start_time": "2021-12-10T02:49:08.659716Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    Collate a list of samples (patient visit sequences) into padded batch tensors.\n",
    "\n",
    "    Each element of `data` is a tuple: (patient_sequence, label)\n",
    "        patient_sequence: list[ list[int] ]  # visits -> diagnosis code ids\n",
    "        label: float or int (heart failure label)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : LongTensor   (B, V_max, C_max)\n",
    "        Padded diagnosis code ids in chronological order.\n",
    "    masks : BoolTensor (B, V_max, C_max)\n",
    "        True where a real code exists, False for padding.\n",
    "    rev_x : LongTensor (B, V_max, C_max)\n",
    "        Same codes but visits reversed in time (compressed to the front, then padded).\n",
    "    rev_masks : BoolTensor (B, V_max, C_max)\n",
    "        Mask for rev_x.\n",
    "    y : FloatTensor (B,)\n",
    "        Labels.\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*data)          # sequences: list of patients; labels: list\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    num_patients = len(sequences)\n",
    "    num_visits_per_patient = [len(p) for p in sequences]\n",
    "    num_codes_all_visits = [len(v) for p in sequences for v in p]\n",
    "\n",
    "    max_num_visits = max(num_visits_per_patient) if num_visits_per_patient else 0\n",
    "    max_num_codes = max(num_codes_all_visits) if num_codes_all_visits else 0\n",
    "\n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros_like(x)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros_like(masks)\n",
    "\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        num_v = len(patient)\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            # Convert visit (list of code ids) to tensor\n",
    "            codes = torch.as_tensor(visit, dtype=torch.long)\n",
    "\n",
    "            # Chronological placement\n",
    "            code_len = len(codes)\n",
    "            x[i_patient, j_visit, :code_len] = codes\n",
    "            masks[i_patient, j_visit, :code_len] = True\n",
    "\n",
    "            # Reversed placement (compress visits to the front in reversed temporal order)\n",
    "            rev_j = num_v - 1 - j_visit\n",
    "            rev_x[i_patient, rev_j, :code_len] = codes\n",
    "            rev_masks[i_patient, rev_j, :code_len] = True\n",
    "\n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, let‚Äôs **walk through this code with a toy dataset** so you can *see exactly* how `collate_fn` transforms the data.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 1: Tiny dataset\n",
    "\n",
    "Let‚Äôs say we have **2 patients** (batch size = 2). Each patient has a different number of visits, and each visit has a different number of diagnosis codes.\n",
    "\n",
    "```python\n",
    "data = [\n",
    "    ( [[1, 2], [3]], 0 ),             # Patient 1 ‚Üí 2 visits ‚Üí label = 0\n",
    "    ( [[4, 5, 6], [7], [8, 9]], 1 )   # Patient 2 ‚Üí 3 visits ‚Üí label = 1\n",
    "]\n",
    "```\n",
    "\n",
    "* Patient 1:\n",
    "\n",
    "  * Visit 1: codes \\[1, 2]\n",
    "  * Visit 2: codes \\[3]\n",
    "  * Label: `0`\n",
    "* Patient 2:\n",
    "\n",
    "  * Visit 1: codes \\[4, 5, 6]\n",
    "  * Visit 2: codes \\[7]\n",
    "  * Visit 3: codes \\[8, 9]\n",
    "  * Label: `1`\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 2: Figure out maximum sizes\n",
    "\n",
    "* Max number of visits = **3** (Patient 2 has 3 visits)\n",
    "* Max number of codes in any visit = **3** (visit \\[4,5,6])\n",
    "\n",
    "So the padded tensors will have shape:\n",
    "\n",
    "```\n",
    "(Batch, MaxVisits, MaxCodes) = (2, 3, 3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 3: Chronological padded tensor `x`\n",
    "\n",
    "Fill each patient‚Äôs visits in order, pad with `0`s.\n",
    "\n",
    "```\n",
    "Patient 1 (2 visits, pad to 3 visits):\n",
    "[[1, 2, 0],    # visit 1\n",
    " [3, 0, 0],    # visit 2\n",
    " [0, 0, 0]]    # padded visit\n",
    "\n",
    "Patient 2 (3 visits, no padding needed):\n",
    "[[4, 5, 6],    # visit 1\n",
    " [7, 0, 0],    # visit 2\n",
    " [8, 9, 0]]    # visit 3\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "x =\n",
    "[\n",
    "  [[1, 2, 0], [3, 0, 0], [0, 0, 0]],\n",
    "  [[4, 5, 6], [7, 0, 0], [8, 9, 0]]\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 4: Chronological mask `masks`\n",
    "\n",
    "Same shape as `x`, with `True` for real codes, `False` for padding.\n",
    "\n",
    "```\n",
    "Patient 1:\n",
    "[[1, 1, 0],   # [1,2]\n",
    " [1, 0, 0],   # [3]\n",
    " [0, 0, 0]]   # padding\n",
    "\n",
    "Patient 2:\n",
    "[[1, 1, 1],   # [4,5,6]\n",
    " [1, 0, 0],   # [7]\n",
    " [1, 1, 0]]   # [8,9]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 5: Reversed visits `rev_x`\n",
    "\n",
    "Visits are flipped in **time order** (most recent first).\n",
    "\n",
    "```\n",
    "Patient 1 reversed (2 visits):\n",
    "[[3, 0, 0],   # visit 2\n",
    " [1, 2, 0],   # visit 1\n",
    " [0, 0, 0]]   # padding\n",
    "\n",
    "Patient 2 reversed (3 visits):\n",
    "[[8, 9, 0],   # visit 3\n",
    " [7, 0, 0],   # visit 2\n",
    " [4, 5, 6]]   # visit 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 6: Reversed mask `rev_masks`\n",
    "\n",
    "```\n",
    "Patient 1:\n",
    "[[1, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0]]\n",
    "\n",
    "Patient 2:\n",
    "[[1, 1, 0],\n",
    " [1, 0, 0],\n",
    " [1, 1, 1]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 7: Labels `y`\n",
    "\n",
    "Simply:\n",
    "\n",
    "```\n",
    "y = [0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Output of `collate_fn`\n",
    "\n",
    "1. `x` ‚Üí padded chronological sequences\n",
    "2. `masks` ‚Üí mask for `x`\n",
    "3. `rev_x` ‚Üí padded reversed sequences\n",
    "4. `rev_masks` ‚Üí mask for `rev_x`\n",
    "5. `y` ‚Üí labels\n",
    "\n",
    "---\n",
    "\n",
    "üìä **Visualization Summary**\n",
    "\n",
    "```\n",
    "Input data (ragged):\n",
    "Patient 1: [[1,2], [3]]                  Label=0\n",
    "Patient 2: [[4,5,6], [7], [8,9]]         Label=1\n",
    "\n",
    "‚Üì Collate_fn pads & masks ‚Üì\n",
    "\n",
    "x = [[[1,2,0], [3,0,0], [0,0,0]],\n",
    "     [[4,5,6], [7,0,0], [8,9,0]]]\n",
    "\n",
    "masks = [[[1,1,0], [1,0,0], [0,0,0]],\n",
    "         [[1,1,1], [1,0,0], [1,1,0]]]\n",
    "\n",
    "rev_x = [[[3,0,0], [1,2,0], [0,0,0]],\n",
    "         [[8,9,0], [7,0,0], [4,5,6]]]\n",
    "\n",
    "rev_masks = [[[1,0,0], [1,1,0], [0,0,0]],\n",
    "             [[1,1,0], [1,0,0], [1,1,1]]]\n",
    "\n",
    "y = [0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also **draw this as a block diagram** (boxes for patients ‚Üí padded batch ‚Üí reversed batch) so you can *see* the transformation flow?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.682628Z",
     "start_time": "2021-12-10T02:49:08.669277Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "249be2d3863d3a27bb7f326e17a872f0",
     "grade": true,
     "grade_id": "cell-4b3472bbf5973793",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == rev_x.dtype == torch.long\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == rev_x.shape == masks.shape == rev_masks.shape == (10, 3, 24)\n",
    "assert y.shape == (10,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf11addba53d9041094d45051435cb7e",
     "grade": false,
     "grade_id": "cell-125312ce2d90406a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we have `CustomDataset` and `collate_fn()`. Let us split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.688088Z",
     "start_time": "2021-12-10T02:49:08.684315Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc4a76fda00ecaef5a2e8857a49fb5f2",
     "grade": false,
     "grade_id": "cell-7f2e734b97c94232",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 800\n",
      "Length of val dataset: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de98a7101e7388850706de0e357437ad",
     "grade": false,
     "grade_id": "cell-c9732f7be72cb6e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 DataLoader [5 points]\n",
    "\n",
    "Now, we can load the dataset into the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.696240Z",
     "start_time": "2021-12-10T02:49:08.692166Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 32\n",
    "    # your code here\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,          # Shuffle only for training\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,         # No shuffle for validation\n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.701374Z",
     "start_time": "2021-12-10T02:49:08.698130Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3e8781ea70db7a1afea7413cef8c3cf",
     "grade": true,
     "grade_id": "cell-0c30a49563819f13",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
    "\n",
    "assert len(train_loader) == 25, \"Length of train_loader should be 25, instead we got %d\"%(len(train_loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c7771a13f558c5045dc0c4d2d2b39c8",
     "grade": false,
     "grade_id": "cell-9739d5ae7e1cafc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2 Naive RNN [35 points] \n",
    "\n",
    "Let us implement a naive bi-directional RNN model.\n",
    "\n",
    "<img src=\"img/bi-rnn.jpg\" width=\"600\"/>\n",
    "\n",
    "Remember from class that, first of all, we need to transform the diagnosis code for each visit of a patient to an embedding. To do this, we can use `nn.Embedding()`, where `num_embeddings` is the number of diagnosis codes and `embedding_dim` is the embedding dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf997f094672a3be91cc5e296d32ba1d",
     "grade": false,
     "grade_id": "cell-7fa15685c339c1c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then, we can construct a simple RNN structure. Each input is this multi-hot vector. At the 0-th visit, this has $\\boldsymbol{X}_0$, and at t-th visit, this has $\\boldsymbol{X}_t$.\n",
    "\n",
    "Each one of the input will then map to a hidden state $\\boldsymbol{\\overleftrightarrow{h}}_t$. The forward hidden state $\\boldsymbol{\\overrightarrow{h}}_t$ can be determined by $\\boldsymbol{\\overrightarrow{h}}_{t-1}$ and the corresponding current input $\\boldsymbol{X}_t$.\n",
    "\n",
    "Similarly, we will have another RNN to process the sequence in the reverse order, so that the hidden state $\\boldsymbol{\\overleftarrow{h}}_t$ is determined by $\\boldsymbol{\\overleftarrow{h}}_{t+1}$ and $\\boldsymbol{X}_t$.\n",
    "\n",
    "Finally, once we have the $\\boldsymbol{\\overrightarrow{h}}_T$ and $\\boldsymbol{\\overleftarrow{h}}_{0}$, we will concatenate the two vectors as the feature vector and train a NN to perform the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70664a7944bd3f61aaf14e3a311e83d4",
     "grade": false,
     "grade_id": "cell-750f2f7f3226048f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let us build this model. The forward steps will be:\n",
    "\n",
    "    1. Pass the sequence through the embedding layer;\n",
    "    2. Sum the embeddings for each diagnosis code up for a visit of a patient;\n",
    "    3. Pass the embeddings through the RNN layer;\n",
    "    4. Obtain the hidden state at the last visit;\n",
    "    5. Do 1-4 for both directions and concatenate the hidden states.\n",
    "    6. Pass the hidden state through the linear and activation layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple explanation:\n",
    "Main Ideas\n",
    "\n",
    "Turn codes into vectors (Embeddings):\n",
    "\n",
    "Each diagnosis code is just a number (like 42 for diabetes).\n",
    "\n",
    "The nn.Embedding() layer converts these numbers into useful vectors (embeddings) so the model can understand them better.\n",
    "\n",
    "Visits = sets of codes:\n",
    "\n",
    "A patient‚Äôs visit has multiple diagnosis codes.\n",
    "\n",
    "We sum all the embeddings for the visit ‚Üí one vector per visit.\n",
    "\n",
    "Process visits in order (RNN):\n",
    "\n",
    "The RNN takes visits one by one (like reading a sentence word by word).\n",
    "\n",
    "At each visit, it keeps track of the current state + previous state.\n",
    "\n",
    "This gives us the forward hidden state (h‚Üít).\n",
    "\n",
    "Process visits in reverse (Bi-RNN):\n",
    "\n",
    "We also process the visits backwards (from last to first).\n",
    "\n",
    "This gives us the backward hidden state (h‚Üêt).\n",
    "\n",
    "Combine both directions:\n",
    "\n",
    "At the end, we take the last forward state and the last backward state.\n",
    "\n",
    "Concatenate (stick them together) ‚Üí one big vector with information from past and future visits.\n",
    "\n",
    "Final classification:\n",
    "\n",
    "Pass this combined vector through a linear layer (like a final decision layer).\n",
    "\n",
    "Use an activation (like sigmoid) to predict if the patient has heart failure (1) or not (0).\n",
    "\n",
    "üß† Think of it like:\n",
    "\n",
    "Embedding layer = dictionary that translates codes into meaningful words.\n",
    "\n",
    "Forward RNN = reading the patient‚Äôs history forward.\n",
    "\n",
    "Backward RNN = reading the history backward.\n",
    "\n",
    "Concatenation = combining both perspectives.\n",
    "\n",
    "Final layer = doctor makes a yes/no decision.\n",
    "\n",
    "üëâ So in short:\n",
    "We embed diagnosis codes ‚Üí summarize visits ‚Üí run them through RNN forward + backward ‚Üí combine ‚Üí classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question üôå Let‚Äôs clear that up.\n",
    "\n",
    "In this **naive Bi-RNN setup**, we **don‚Äôt use all visits‚Äô hidden states**. Instead:\n",
    "\n",
    "* We process all visits forward ‚Üí at the **last visit** we get the **last forward hidden state** (`h‚ÜíT`).\n",
    "* We process all visits backward ‚Üí at the **first visit** we get the **last backward hidden state** (`h‚Üê0`).\n",
    "\n",
    "üëâ Then we **only take these two final states** (not all visits), concatenate them:\n",
    "\n",
    "```\n",
    "h_final = [ h‚ÜíT ; h‚Üê0 ]- thhis is the summary of all visit\n",
    "```\n",
    "\n",
    "and pass that through the **Linear layer** for classification.\n",
    "\n",
    "---\n",
    "\n",
    "üîë Why?\n",
    "Because this model just wants a **summary of the whole patient history** (forward & backward), not predictions at each visit.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ But in more advanced models (like sequence-to-sequence, attention, or Transformers), yes‚Äîyou often combine **all hidden states from every visit** before classification.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2aaf3a348d4abc77607b8b288b56bb8b",
     "grade": false,
     "grade_id": "cell-a9cb7f4d8889ca27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1 Mask Selection [20 points]\n",
    "\n",
    "Importantly, you need to use `masks` to mask out the paddings in before step 2 and before 4. So, let us first preform the mask selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.705960Z",
     "start_time": "2021-12-10T02:49:08.703151Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "     # Ensure boolean -> float for multiplication (broadcast over embedding_dim)\n",
    "    mask_float = masks.unsqueeze(-1).to(dtype=x.dtype)          # (B, V, C, 1)\n",
    "\n",
    "    # Zero out padded positions then sum across code dimension\n",
    "    sum_embeddings = (x * mask_float).sum(dim=2)\n",
    "    \n",
    "    return sum_embeddings\n",
    "\n",
    "    #raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how this function work with real data\n",
    "Perfect üëç Let‚Äôs **walk through this with a real mini dataset** so you can see how `sum_embeddings_with_mask()` works step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Recall what the function does\n",
    "\n",
    "* `x`: embeddings for each diagnosis code ‚Üí shape = `(B, V, C, D)`\n",
    "\n",
    "  * `B = batch size (patients)`\n",
    "  * `V = number of visits`\n",
    "  * `C = number of codes per visit`\n",
    "  * `D = embedding dimension`\n",
    "* `masks`: tells us which entries are **real codes (1)** vs \\*\\*padding (0)`. Shape = `(B, V, C)\\`.\n",
    "* Output: sum embeddings per visit ‚Üí `(B, V, D)`.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Example data\n",
    "\n",
    "Let‚Äôs say:\n",
    "\n",
    "* Batch size = 1 patient (`B=1`)\n",
    "* Visits = 2 (`V=2`)\n",
    "* Codes per visit = 3 (`C=3`)\n",
    "* Embedding dimension = 2 (`D=2`)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Embeddings (B=1, V=2, C=3, D=2)\n",
    "x = torch.tensor([\n",
    "    [   # Patient 1\n",
    "        [[1., 1.], [2., 2.], [0., 0.]],   # Visit 1: two real codes, one padding\n",
    "        [[3., 3.], [4., 4.], [5., 5.]]    # Visit 2: three real codes\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Masks (B=1, V=2, C=3)\n",
    "masks = torch.tensor([\n",
    "    [\n",
    "        [1, 1, 0],   # Visit 1: first two codes real, last one padding\n",
    "        [1, 1, 1]    # Visit 2: all three codes real\n",
    "    ]\n",
    "])\n",
    "\n",
    "def sum_embeddings_with_mask(x, masks):\n",
    "    mask_float = masks.unsqueeze(-1).to(dtype=x.dtype)   # (B, V, C, 1)\n",
    "    sum_embeddings = (x * mask_float).sum(dim=2)        # sum over codes\n",
    "    return sum_embeddings\n",
    "\n",
    "sum_embeddings_with_mask(x, masks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What happens?\n",
    "\n",
    "1. `x` (embeddings):\n",
    "\n",
    "   ```\n",
    "   Visit 1: [[1,1], [2,2], [0,0]]  \n",
    "   Visit 2: [[3,3], [4,4], [5,5]]  \n",
    "   ```\n",
    "\n",
    "2. `masks`:\n",
    "\n",
    "   ```\n",
    "   Visit 1: [1, 1, 0]  (last code ignored)  \n",
    "   Visit 2: [1, 1, 1]  (all used)  \n",
    "   ```\n",
    "\n",
    "3. Multiply `x * masks`:\n",
    "\n",
    "   ```\n",
    "   Visit 1: [[1,1], [2,2], [0,0]]  \n",
    "   Visit 2: [[3,3], [4,4], [5,5]]  \n",
    "   ```\n",
    "\n",
    "   (nothing changes for real codes, padding stays zero)\n",
    "\n",
    "4. Sum across codes (`dim=2`):\n",
    "\n",
    "   ```\n",
    "   Visit 1 sum: [3,3]   ( [1,1] + [2,2] )  \n",
    "   Visit 2 sum: [12,12] ( [3,3] + [4,4] + [5,5] )  \n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Final Output\n",
    "\n",
    "Shape = `(B=1, V=2, D=2)`\n",
    "\n",
    "```\n",
    "tensor([[\n",
    "    [ 3.,  3.],   # Visit 1\n",
    "    [12., 12.]    # Visit 2\n",
    "]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üëâ So the function **zeroes out padded codes** and **sums only the real code embeddings per visit**.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also show this with a **batch of 2 patients**, so you see how it works across multiple patients at once?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.769541Z",
     "start_time": "2021-12-10T02:49:08.707555Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf2ca473060eda35326593899a7fb2ae",
     "grade": true,
     "grade_id": "cell-e2e6868f7bd913f8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "\n",
    "\n",
    "def uses_loop(function):\n",
    "    loop_statements = ast.For, ast.While, ast.AsyncFor\n",
    "\n",
    "    nodes = ast.walk(ast.parse(inspect.getsource(function)))\n",
    "    return any(isinstance(node, loop_statements) for node in nodes)\n",
    "\n",
    "def generate_random_mask(batch_size, max_num_visits , max_num_codes):\n",
    "    num_visits = [random.randint(1, max_num_visits) for _ in range(batch_size)]\n",
    "    num_codes = []\n",
    "    for n in num_visits:\n",
    "        num_codes_visit = [0] * max_num_visits\n",
    "        for i in range(n):\n",
    "            num_codes_visit[i] = (random.randint(1, max_num_codes))\n",
    "        num_codes.append(num_codes_visit)\n",
    "    masks = [torch.ones((l,), dtype=torch.bool) for num_codes_visit in num_codes for l in num_codes_visit]\n",
    "    masks = torch.stack([torch.cat([i, i.new_zeros(max_num_codes - i.size(0))], 0) for i in masks], 0)\n",
    "    masks = masks.view((batch_size, max_num_visits, max_num_codes)).bool()\n",
    "    return masks\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "max_num_visits = 10\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "x = torch.randn((batch_size, max_num_visits , max_num_codes, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = sum_embeddings_with_mask(x, masks)\n",
    "\n",
    "assert uses_loop(sum_embeddings_with_mask) is False\n",
    "assert out.shape == (batch_size, max_num_visits, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.774272Z",
     "start_time": "2021-12-10T02:49:08.770811Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    Obtain the hidden state for the LAST real (non‚Äëpadding) visit for each patient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_states : torch.Tensor\n",
    "        Shape (B, V, E) where\n",
    "          B = batch size\n",
    "          V = max # visits (padded)\n",
    "          E = embedding dim\n",
    "        Each row along V corresponds to a visit's hidden state (already aggregated per visit).\n",
    "    masks : torch.Tensor (bool)\n",
    "        Shape (B, V, C) visit-level padding mask over diagnosis codes.\n",
    "        A visit is considered 'real' if ANY code in that visit is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    last_hidden_state : torch.Tensor\n",
    "        Shape (B, E) containing the hidden state of the last real visit for every patient.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - No Python for-loops are used.\n",
    "    - Uses torch.gather as hinted.\n",
    "    - Assumes that padded visits (all-False) appear only after real visits (standard padding).\n",
    "      If there are interior empty visits, the function still picks the last visit that has at least\n",
    "      one True code.\n",
    "    \"\"\"\n",
    "    # 1. Derive a (B, V) boolean mask indicating which visits are real\n",
    "    visit_mask = masks.any(dim=2)                   # (B, V)\n",
    "\n",
    "    # 2. Build an index tensor of visit positions 0..V-1\n",
    "    B, V, E = hidden_states.shape\n",
    "    positions = torch.arange(V, device=hidden_states.device).unsqueeze(0)  # (1, V)\n",
    "\n",
    "    # 3. Zero out positions where visit is padding (or set to -1) to ignore them.\n",
    "    # Using -1 ensures they won't become the max unless all visits are padding.\n",
    "    masked_positions = positions.masked_fill(~visit_mask, -1)\n",
    "\n",
    "    # 4. Find the index of the last real visit (largest position that is real)\n",
    "    last_indices = masked_positions.argmax(dim=1)   # (B,)\n",
    "\n",
    "    # (Optional) If you want to guard against patients with no real visits, you can add:\n",
    "    # if (visit_mask.sum(dim=1) == 0).any():\n",
    "    #     raise ValueError(\"At least one patient has no real (non-padding) visits.\")\n",
    "\n",
    "    # 5. Gather the hidden states at those indices\n",
    "    gather_index = last_indices.view(B, 1, 1).expand(B, 1, E)  # (B, 1, E)\n",
    "    last_hidden_state = hidden_states.gather(dim=1, index=gather_index).squeeze(1)  # (B, E)\n",
    "\n",
    "    return last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.788183Z",
     "start_time": "2021-12-10T02:49:08.775986Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a57f7f7d77bd03e1d4f0b3e53cca7ec7",
     "grade": true,
     "grade_id": "cell-611bb60b8cff5f77",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert uses_loop(get_last_visit) is False\n",
    "\n",
    "max_num_visits = 10\n",
    "batch_size = 16\n",
    "max_num_codes = 20\n",
    "embedding_dim = 100\n",
    "\n",
    "torch.random.manual_seed(7)\n",
    "hidden_states = torch.randn((batch_size, max_num_visits, embedding_dim))\n",
    "masks = generate_random_mask(batch_size, max_num_visits , max_num_codes)\n",
    "out = get_last_visit(hidden_states, masks)\n",
    "\n",
    "assert out.shape == (batch_size, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lat visit calculation\n",
    "Perfect! Let‚Äôs go **step by step** with the 4-patient toy data we used earlier. This will show exactly how `get_last_visit()` works.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recall the data\n",
    "\n",
    "### Hidden states (summed embeddings per visit) = `hidden_states`\n",
    "\n",
    "Shape = `(B=4, V=2, E=2)`\n",
    "\n",
    "```\n",
    "Patient 1 ‚Üí [[ 3,  3], [12, 12]]\n",
    "Patient 2 ‚Üí [[ 1,  1], [13, 13]]\n",
    "Patient 3 ‚Üí [[ 9,  9], [ 0,  0]]\n",
    "Patient 4 ‚Üí [[ 5,  5], [ 6,  6]]\n",
    "```\n",
    "\n",
    "### Masks (`masks`)\n",
    "\n",
    "Shape = `(B=4, V=2, C=3)`\n",
    "\n",
    "```\n",
    "Patient 1: [[1,1,0], [1,1,1]]\n",
    "Patient 2: [[1,0,0], [1,1,0]]\n",
    "Patient 3: [[1,1,1], [0,0,0]]\n",
    "Patient 4: [[1,0,0], [1,1,1]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Step 1: Determine which visits are real\n",
    "\n",
    "```python\n",
    "visit_mask = masks.any(dim=2)   # (B, V)\n",
    "```\n",
    "\n",
    "* `True` if **any code in the visit is real**\n",
    "\n",
    "```\n",
    "Patient 1: [True, True]      # both visits have codes\n",
    "Patient 2: [True, True]\n",
    "Patient 3: [True, False]     # second visit is padding\n",
    "Patient 4: [True, True]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step 2 & 3: Build positions and mask padding\n",
    "\n",
    "```python\n",
    "positions = [[0,1]]   # visit indices along dim=1\n",
    "masked_positions = positions.masked_fill(~visit_mask, -1)\n",
    "```\n",
    "\n",
    "After masking padded visits (False ‚Üí -1):\n",
    "\n",
    "```\n",
    "Patient 1: [0,1]\n",
    "Patient 2: [0,1]\n",
    "Patient 3: [0,-1]\n",
    "Patient 4: [0,1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Step 4: Find last real visit\n",
    "\n",
    "```python\n",
    "last_indices = masked_positions.argmax(dim=1)  # largest valid index\n",
    "```\n",
    "\n",
    "* Picks **largest position that is real** (last visit)\n",
    "\n",
    "```\n",
    "Patient 1: 1  # Visit 2\n",
    "Patient 2: 1  # Visit 2\n",
    "Patient 3: 0  # Visit 1\n",
    "Patient 4: 1  # Visit 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Step 5: Gather hidden states\n",
    "\n",
    "```python\n",
    "gather_index = last_indices.view(B,1,1).expand(B,1,E)\n",
    "last_hidden_state = hidden_states.gather(dim=1, index=gather_index).squeeze(1)\n",
    "```\n",
    "\n",
    "* Picks hidden state at **last real visit** for each patient\n",
    "\n",
    "```\n",
    "Patient 1: [12, 12]  # Visit 2\n",
    "Patient 2: [13, 13]  # Visit 2\n",
    "Patient 3: [9, 9]    # Visit 1 (second is padding)\n",
    "Patient 4: [6, 6]    # Visit 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Output (`last_hidden_state`)\n",
    "\n",
    "```\n",
    "tensor([\n",
    " [12, 12],\n",
    " [13, 13],\n",
    " [ 9,  9],\n",
    " [ 6,  6]\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Summary\n",
    "\n",
    "1. `masks.any(dim=2)` ‚Üí figure out which visits have real codes.\n",
    "2. Build visit **indices**, mask out padded visits with -1.\n",
    "3. `argmax` ‚Üí find **last real visit**.\n",
    "4. `gather` ‚Üí pick hidden states at those indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62bbd9209d97a0d0469cb4f9ff414d45",
     "grade": false,
     "grade_id": "cell-51a88c33b34e6827",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 Build NaiveRNN [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.807198Z",
     "start_time": "2021-12-10T02:49:08.789522Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(619, 128, padding_idx=0)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class NaiveRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple dual-direction (manual bidirectional) RNN model:\n",
    "      1. Embed diagnosis codes.\n",
    "      2. Sum code embeddings within each visit (masking padding codes).\n",
    "      3. Run a forward GRU over visits.\n",
    "      4. Run a reverse GRU over visits constructed from reversed visit order.\n",
    "      5. Extract the last real visit hidden state from each direction.\n",
    "      6. Concatenate forward + reverse states (size 256) -> Linear -> Sigmoid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        embDimSize = 128\n",
    "        hidden_size = 128\n",
    "\n",
    "        # 1. Embedding layer (assumes 0 is padding index)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_codes,\n",
    "            embedding_dim=embDimSize,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        # 2. Forward GRU (visit-level)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=embDimSize,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 3. Reverse GRU (will process reversed visit order)\n",
    "        self.rev_rnn = nn.GRU(\n",
    "            input_size=embDimSize,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 4. Linear head (concat forward + reverse -> 256 -> 1)\n",
    "        self.fc = nn.Linear(2 * hidden_size, 1)\n",
    "\n",
    "        # 5. Sigmoid activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        x:        (B, V, C) diagnosis code indices (forward order)\n",
    "        masks:    (B, V, C) boolean mask for x\n",
    "        rev_x:    (B, V, C) diagnosis code indices (reversed visits)\n",
    "        rev_masks:(B, V, C) boolean mask for rev_x\n",
    "\n",
    "        Returns:\n",
    "            probs: (B,) probability per patient\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # ----- Forward direction -----\n",
    "        # Step 1: embed\n",
    "        emb_forward = self.embedding(x)              # (B, V, C, E)\n",
    "        # Step 2: sum code embeddings per visit\n",
    "        visit_emb_forward = sum_embeddings_with_mask(emb_forward, masks)   # (B, V, E)\n",
    "        # Step 3: GRU over visits\n",
    "        output_forward, _ = self.rnn(visit_emb_forward)                     # (B, V, H)\n",
    "        # Step 4: last real visit hidden state\n",
    "        true_h_n_forward = get_last_visit(output_forward, masks)            # (B, H)\n",
    "\n",
    "        # ----- Reverse direction (same steps with reversed visit order) -----\n",
    "        emb_reverse = self.embedding(rev_x)                                 # (B, V, C, E)\n",
    "        visit_emb_reverse = sum_embeddings_with_mask(emb_reverse, rev_masks)# (B, V, E)\n",
    "        output_reverse, _ = self.rev_rnn(visit_emb_reverse)                 # (B, V, H)\n",
    "        true_h_n_reverse = get_last_visit(output_reverse, rev_masks)        # (B, H)\n",
    "\n",
    "        # Concatenate forward + reverse final states -> (B, 2H)\n",
    "        concat_h = torch.cat([true_h_n_forward, true_h_n_reverse], dim=1)    # (B, 256)\n",
    "\n",
    "        # Linear + sigmoid\n",
    "        logits = self.fc(concat_h)                                          # (B, 1)\n",
    "        probs = self.sigmoid(logits).view(batch_size)                       # (B,)\n",
    "      \n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(types))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.812597Z",
     "start_time": "2021-12-10T02:49:08.808592Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1da92a4649f26a9fc60f4d6ded75c7ba",
     "grade": true,
     "grade_id": "cell-45de4813453d610f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAUTOGRADER CELL. DO NOT MODIFY THIS.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.822685Z",
     "start_time": "2021-12-10T02:49:08.814539Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c4bf456dd6172e11a5b2038801aedda",
     "grade": true,
     "grade_id": "cell-0c00e2f67834bd8d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAUTOGRADER CELL. DO NOT MODIFY THIS.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f778ecb7179f34af1319d5e03d7ee599",
     "grade": false,
     "grade_id": "cell-3b34f300dcde3d1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3 Model Training [35 points]\n",
    "\n",
    "### 3.1 Loss and Optimizer [5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.827118Z",
     "start_time": "2021-12-10T02:49:08.823986Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Specify Binary Cross Entropy as the loss function (`nn.BCELoss`) and assign it to `criterion`.\n",
    "      Spcify Adam as the optimizer (`torch.optim.Adam`)  with learning rate 0.001 and assign it to `optimizer`.\n",
    "\"\"\"\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# your code here\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:08.831838Z",
     "start_time": "2021-12-10T02:49:08.828878Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f642b0163a31499897a3b2a3e66657f6",
     "grade": true,
     "grade_id": "cell-40bee829e3b63b7d",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAUTOGRADER CELL. DO NOT MODIFY THIS.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eab2b782149c6d539961da3b7f25ab09",
     "grade": false,
     "grade_id": "cell-873df7380d762445",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Evaluate [10 points]\n",
    "\n",
    "Then, let us implement the `eval_model()` function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:09.238600Z",
     "start_time": "2021-12-10T02:49:08.833460Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "  \n",
    "    # your code here\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    #raise NotImplementedError\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:09.409363Z",
     "start_time": "2021-12-10T02:49:09.240136Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "197e373f11419054f6005341d13867bb",
     "grade": true,
     "grade_id": "cell-764df4f66a8f01e4",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "p, r, f, roc_auc = eval_model(naive_rnn, val_loader)\n",
    "assert p.size == 1, \"Precision should be a scalar.\"\n",
    "assert r.size == 1, \"Recall should be a scalar.\"\n",
    "assert f.size == 1, \"F1 should be a scalar.\"\n",
    "assert roc_auc.size == 1, \"ROC-AUC should be a scalar.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f2026101cfd198567edf0c5d2fe71b8",
     "grade": false,
     "grade_id": "cell-9b3672b70944a8d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.3 Training and evlauation [20 points]\n",
    "\n",
    "Now let us implement the `train()` function. Note that `train()` should call `eval_model()` at the end of each training epoch to see the results on the validaion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:09.415838Z",
     "start_time": "2021-12-10T02:49:09.410578Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            # 1. zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. forward\n",
    "            y_hat = model(x, masks, rev_x, rev_masks)\n",
    "            \n",
    "            # 3. compute loss\n",
    "            loss = loss_fn(y_hat, y.float())\n",
    "            \n",
    "            # 4. backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # accumulate\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1} \\t Training Loss: {train_loss:.6f}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print(f'Epoch: {epoch+1} \\t Validation p: {p:.2f}, r:{r:.2f}, f: {f:.2f}, roc_auc: {roc_auc:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:12.664804Z",
     "start_time": "2021-12-10T02:49:09.417438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.624361\n",
      "Epoch: 1 \t Validation p: 0.71, r:0.89, f: 0.79, roc_auc: 0.84\n",
      "Epoch: 2 \t Training Loss: 0.441252\n",
      "Epoch: 2 \t Validation p: 0.70, r:0.86, f: 0.77, roc_auc: 0.84\n",
      "Epoch: 3 \t Training Loss: 0.329138\n",
      "Epoch: 3 \t Validation p: 0.74, r:0.86, f: 0.80, roc_auc: 0.85\n",
      "Epoch: 4 \t Training Loss: 0.224841\n",
      "Epoch: 4 \t Validation p: 0.70, r:0.86, f: 0.77, roc_auc: 0.84\n",
      "Epoch: 5 \t Training Loss: 0.140503\n",
      "Epoch: 5 \t Validation p: 0.78, r:0.83, f: 0.81, roc_auc: 0.86\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:12.792877Z",
     "start_time": "2021-12-10T02:49:12.666169Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2ab4e44c6b1125682a2d8ca02030673",
     "grade": true,
     "grade_id": "cell-8fc0a72d1a31aa34",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585727154438996\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "p, r, f, roc_auc = eval_model(naive_rnn, val_loader)\n",
    "print(roc_auc)\n",
    "assert roc_auc > 0.7, \"ROC AUC is too low on the validation set (%f < 0.7)\"%(roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T02:49:12.869415Z",
     "start_time": "2021-12-10T02:49:12.794442Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ed19dc555eb7ae8c97e038425457774",
     "grade": true,
     "grade_id": "cell-8e9d1d7cb1c3a386",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAUTOGRADER CELL. DO NOT MODIFY THIS.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect üëç Let‚Äôs actually **run through the example step by step** with the simple 3-patient dataset so you can see how the `NaiveRNN` processes it.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Patients and their visits\n",
    "\n",
    "We‚Äôll use codes `{0:PAD, 1:Hypertension, 2:Diabetes, 3:CHF, 4:CAD, 5:CKD}`\n",
    "\n",
    "* **Patient 1** ‚Üí Visit 1: \\[HTN, Diabetes], Visit 2: \\[CHF]\n",
    "* **Patient 2** ‚Üí Visit 1: \\[CAD], Visit 2: \\[HTN, CHF, CKD]\n",
    "* **Patient 3** ‚Üí Visit 1: \\[Diabetes], Visit 2: \\[HTN], Visit 3: \\[CHF, CAD]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Padding into tensors\n",
    "\n",
    "We need to fit them into tensors `(B, V, C)` = `(batch, visits, codes_per_visit)`\n",
    "\n",
    "| Patient | Visits (after padding)                |\n",
    "| ------- | ------------------------------------- |\n",
    "| P1      | \\[\\[1, 2, 0], \\[3, 0, 0], \\[0, 0, 0]] |\n",
    "| P2      | \\[\\[4, 0, 0], \\[1, 3, 5], \\[0, 0, 0]] |\n",
    "| P3      | \\[\\[2, 0, 0], \\[1, 0, 0], \\[3, 4, 0]] |\n",
    "\n",
    "Here `0` is just padding.\n",
    "We also create **masks** (1 where real code, 0 where padded).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What the model does\n",
    "\n",
    "1. **Embedding layer**\n",
    "   Each code gets mapped to a 128-dim vector.\n",
    "   ‚Üí Now each visit looks like a set of 128-dim vectors.\n",
    "\n",
    "2. **Sum per visit**\n",
    "   For example, Visit `[1, 2]` (HTN + Diabetes) becomes `embedding(1) + embedding(2)`.\n",
    "   So each visit becomes a single 128-dim vector.\n",
    "\n",
    "3. **Forward GRU**\n",
    "   Reads visit embeddings in order: V1 ‚Üí V2 ‚Üí V3.\n",
    "   Produces hidden states per visit, keeps last valid visit.\n",
    "\n",
    "4. **Reverse GRU**\n",
    "   Reads visits in reverse: V3 ‚Üí V2 ‚Üí V1.\n",
    "   Same process, keeps last valid visit (which corresponds to the original first).\n",
    "\n",
    "5. **Concatenate last forward + reverse states** ‚Üí (256-dim).\n",
    "\n",
    "6. **Linear + Sigmoid** ‚Üí Gives probability of outcome (heart failure risk).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Running the model\n",
    "\n",
    "Here‚Äôs the exact code you can run to see the predictions:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    probs = naive_rnn(x, masks, rev_x, rev_masks)\n",
    "\n",
    "print(\"Prediction probabilities per patient:\")\n",
    "for i, p in enumerate(probs):\n",
    "    print(f\"Patient {i+1}: {p.item():.4f}\")\n",
    "```\n",
    "\n",
    "üîπ Example output (since weights are random, numbers will vary):\n",
    "\n",
    "```\n",
    "Patient 1: 0.4871\n",
    "Patient 2: 0.5123\n",
    "Patient 3: 0.4950\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üëâ So what you see is:\n",
    "\n",
    "* Each patient gets a **probability score between 0 and 1**.\n",
    "* With real training, this score would represent their **risk of heart failure** given their visit history.\n",
    "* Right now, since the model is untrained, it outputs \\~0.5 for all (randomly).\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to also **visualize one patient‚Äôs forward & reverse GRU hidden states** (so you can literally see how information flows visit by visit)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW3_RNN/HW3_RNN.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
