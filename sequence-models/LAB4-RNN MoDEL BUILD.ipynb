{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66e7f07ff8da05c0613f4335a5916f7b",
     "grade": false,
     "grade_id": "cell-67ef996f07aa3ec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab 4\n",
    "\n",
    "In the previous labs, we perform mortality prediction using DNN and CNN. However, to deal with sequential data, the most commonly used architecture is actually recurrent neural network (RNN). This lab introduces you to the motivation, implementation, and some commonly used RNN models. Let us get started!\n",
    "\n",
    "Table of Contents:\n",
    "- Motivation\n",
    "- Implementation\n",
    "- Modern RNN Models\n",
    "- Assignment\n",
    "\n",
    "Some contents of this lab are adapted from [Dive into Deep Learning](https://d2l.ai) and [Official PyTorch Tutorials](https://pytorch.org/tutorials/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.121633Z",
     "start_time": "2021-12-10T04:02:05.517588Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7886bdee48e2d70080bf8171b533d19",
     "grade": false,
     "grade_id": "cell-05b5cd4b51c7c115",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os, random ‚Üí standard Python tools.\n",
    "\n",
    "numpy ‚Üí math & arrays.\n",
    "\n",
    "torch, torch.nn, torch.nn.functional ‚Üí deep learning.\n",
    "\n",
    "pandas ‚Üí working with data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.128075Z",
     "start_time": "2021-12-10T04:02:06.123724Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c20a6b2d5dcf1ff1b54a78787843f632",
     "grade": false,
     "grade_id": "cell-c136bff551290161",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training AI models, randomness happens everywhere ‚Äî in data shuffling, weight initialization, augmentations, etc.\n",
    "If you set the seed like this, your experiments are repeatable.\n",
    "Example: If you train today and tomorrow with the same code and data, you get exactly the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.254468Z",
     "start_time": "2021-12-10T04:02:06.129895Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "285586abd7789a119de4008c0a716510",
     "grade": false,
     "grade_id": "cell-13c70c678cbe91b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.csv\r\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../LAB4-lib/data\"\n",
    "assert os.path.isdir(DATA_PATH)\n",
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÄúHere‚Äôs the box path where I keep my datadoler. Check that it exists, if it , continue, then open it and tell me what‚Äôs inside.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a240d9088d7848bd7975b4bfccd2460b",
     "grade": false,
     "grade_id": "cell-fccc65aaca1f8dfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Motivation\n",
    "\n",
    "While CNNs can efficiently process spatial information, recurrent neural networks (RNNs) are designed to better handle sequential information. RNNs introduce state variables to store past information, together with the current inputs, to determine the current outputs.\n",
    "\n",
    "That is, let us say we have a sequence $x_1, x_2, \\dots, x_t$. RNN attempts to model the conditional probability: $P(x_t \\mid x_1, x_2, \\dots, x_{t-1})$. More specifically, RNN leverages a hidden state variable  that stores the sequence information up to time step $t‚àí1$:\n",
    "\n",
    "$$P(x_t \\mid x_{1}, x_{2}, \\ldots, x_{t-1}) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "where $h_{t-1}$ is the hidden state variable. In general, the hidden state at any time step $t$ could be computed based on both the current input $x_t$ and the previous hidden state $h_{t-1}$:\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here‚Äôs the short version of what I just explained:\n",
    "\n",
    "CNNs are great for spatial data (like images) but don‚Äôt naturally handle sequences where order matters.\n",
    "\n",
    "RNNs are designed for sequences ‚Äî they remember past information using a hidden state.\n",
    "\n",
    "They model the probability of the current input given all previous inputs-ùëÉ(ùë•ùë°‚à£ùë•1,ùë•2,‚Ä¶,ùë•ùë°‚àí1)‚âàùëÉ(ùë•ùë°‚à£‚Ñéùë°‚àí1),\n",
    "where ‚Ñéùë°‚àí1 is the hidden state variable. In general, the hidden state at any time step ùë° could be computed based on both the current input ùë•ùë° and the previous hidden state ‚Ñéùë°‚àí1\n",
    "\n",
    ":\n",
    "\n",
    "‚Ñéùë°=ùëì(ùë•ùë°,‚Ñéùë°‚àí1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe092d44760bec7df70df26af07e6c6a",
     "grade": false,
     "grade_id": "cell-85a8813f852c2ba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "Assume that we have a minibatch of inputs $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ at time step $t$. In other words, for a minibatch of $n$ sequence examples, each row of $\\mathbf{X}_t$ corresponds to one example at time step $t$ from the sequence. Next, denote by $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$ the hidden variable of time step $t$. Further, we save the hidden variable $\\mathbf{H}_{t-1}$ from the previous time step and introduce a new weight parameter $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ to describe how to use the hidden variable of the previous time step in the current time step. Specifically, the calculation of the hidden variable of the current time step is determined by the input of the current time step together with the hidden variable of the previous time step:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    "\n",
    "From the relationship between hidden variables $\\mathbf{H}_{t}$ and $\\mathbf{H}_{t-1}$ of adjacent time steps, we know that these variables captured and retained the sequence‚Äôs historical information up to their current time step, just like the state or memory of the neural network‚Äôs current time step. Therefore, such a hidden variable is called a hidden state. Since the hidden state uses the same definition of the previous time step in the current time step, the computation of $\\mathbf{H}_{t}$ is recurrent. Hence, neural networks with hidden states based on recurrent computation are named recurrent neural networks. Layers that perform the computation of $\\mathbf{H}_{t}$ in RNNs are called recurrent layers.\n",
    "\n",
    "For time step $t$, the output of the output layer is similar to the computation in the MLP:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "Parameters of the RNN include the weights $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$, and the bias $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$ of the hidden layer, together with the weights $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ and the bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ of the output layer. It is worth mentioning that even at different time steps, RNNs always use these model parameters. Therefore, the parameterization cost of an RNN does not grow as the number of time steps increases.\n",
    "\n",
    "<img src='img/rnn.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and Hidden States\n",
    "\n",
    "At each time step \n",
    "t\n",
    "t, the RNN takes:\n",
    "\n",
    "The current input \n",
    "Xt\n",
    "X\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    " (data at this time step)\n",
    "\n",
    "The hidden state \n",
    "Ht‚àí1\n",
    "H\n",
    "t‚àí1\n",
    "\t‚Äã\n",
    "\n",
    " from the previous time step (memory from the past)\n",
    "\n",
    "Updating the Hidden State\n",
    "\n",
    "The new hidden state \n",
    "Ht\n",
    "H\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    " is calculated by combining:\n",
    "\n",
    "The current input \n",
    "Xt\n",
    "X\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    " multiplied by input weights \n",
    "Wxh\n",
    "W\n",
    "xh\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "The previous hidden state \n",
    "Ht‚àí1\n",
    "H\n",
    "t‚àí1\n",
    "\t‚Äã\n",
    "\n",
    " multiplied by recurrent weights \n",
    "Whh\n",
    "W\n",
    "hh\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "A bias term \n",
    "bh\n",
    "b\n",
    "h\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "Then applying an activation function \n",
    "œï\n",
    "œï (e.g., tanh or ReLU).\n",
    "\n",
    "Formula:\n",
    "\n",
    "Ht=œï(XtWxh+Ht‚àí1Whh+bh)\n",
    "H\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    "=œï(X\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    "W\n",
    "xh\n",
    "\t‚Äã\n",
    "\n",
    "+H\n",
    "t‚àí1\n",
    "\t‚Äã\n",
    "\n",
    "W\n",
    "hh\n",
    "\t‚Äã\n",
    "\n",
    "+b\n",
    "h\n",
    "\t‚Äã\n",
    "\n",
    ")\n",
    "\n",
    "Why It‚Äôs Called ‚ÄúRecurrent‚Äù\n",
    "\n",
    "The same formula is used at every time step.\n",
    "\n",
    "The hidden state carries historical information through the sequence ‚Äî acting like memory.\n",
    "\n",
    "Output at Each Step\n",
    "\n",
    "The RNN produces an output \n",
    "Ot\n",
    "O\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    " from the hidden state:\n",
    "\n",
    "Ot=HtWhq+bq\n",
    "O\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    "=H\n",
    "t\n",
    "\t‚Äã\n",
    "\n",
    "W\n",
    "hq\n",
    "\t‚Äã\n",
    "\n",
    "+b\n",
    "q\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "Parameters Are Shared Across Time\n",
    "\n",
    "The weights and biases (\n",
    "Wxh,Whh,Whq,bh,bq\n",
    "W\n",
    "xh\n",
    "\t‚Äã\n",
    "\n",
    ",W\n",
    "hh\n",
    "\t‚Äã\n",
    "\n",
    ",W\n",
    "hq\n",
    "\t‚Äã\n",
    "\n",
    ",b\n",
    "h\n",
    "\t‚Äã\n",
    "\n",
    ",b\n",
    "q\n",
    "\t‚Äã\n",
    "\n",
    ") stay the same for every time step.\n",
    "\n",
    "This makes RNNs efficient because the number of parameters doesn‚Äôt grow with sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1bd296b858a8733a760442ee5cdb78e",
     "grade": false,
     "grade_id": "cell-bd33ea245d64c9d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1 [20 points]\n",
    "\n",
    "Implement the function calculating the current hidden state:\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h.$$\n",
    "\n",
    "Note that here `X` is the input at a sinlge time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.262258Z",
     "start_time": "2021-12-10T04:02:06.258101Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def calculate_current_h(X, H, W_xh, W_hh, b_h):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - X: (batch size, input dimension)\n",
    "        - H: (batch size, hidden dimension)\n",
    "        - W_xh: (input dimension, hidden dimension)\n",
    "        - W_hh: (hidden dimension, hidden dimension)\n",
    "        - b_h: (1, hidden dimension)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    H_t = X @ W_xh + H @ W_hh + b_h\n",
    "    #raise NotImplementedError\n",
    "    return H_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.273316Z",
     "start_time": "2021-12-10T04:02:06.264326Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "114fe7beef2fac92227bb321c2369f16",
     "grade": true,
     "grade_id": "cell-1c5b169c773b6b5d",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n, d, h = 2, 8, 4 # batch size, input dimension, hidden dimension\n",
    "X, W_xh = torch.normal(0, 1, (n, d)), torch.normal(0, 1, (d, h))\n",
    "H, W_hh = torch.normal(0, 1, (n, h)), torch.normal(0, 1, (h, h))\n",
    "b_h = torch.normal(0, 1, (1, h))\n",
    "assert torch.allclose(calculate_current_h(X, H, W_xh, W_hh, b_h),\n",
    "                      torch.tensor([[ 5.1226, -0.6884,  3.1821,  6.6513],\n",
    "                                    [-0.6077,  2.2313, -1.4812,  0.4403]]), rtol=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1352b212df3fdaf5b6acc167e99d6b53",
     "grade": false,
     "grade_id": "cell-b1abb8914e8b246e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2 [20 points]\n",
    "\n",
    "Call the previous implemented `calculate_current_h` recursively to calculate the final hidden state.\n",
    "\n",
    "Note that here `inputs` is the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.278148Z",
     "start_time": "2021-12-10T04:02:06.274869Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def rnn(inputs, state, W_xh, W_hh, b_h):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        - inputs: (batch size, sequence length, input dimension)\n",
    "        - state: (batch size, hidden dimension)  # initial hidden state H0\n",
    "        - W_xh: (input dimension, hidden dimension)\n",
    "        - W_hh: (hidden dimension, hidden dimension)\n",
    "        - b_h: (1, hidden dimension)\n",
    "    Returns:\n",
    "        - final hidden state after processing the sequence\n",
    "    \"\"\"\n",
    "    H = state  # start from initial hidden state\n",
    "\n",
    "    # Loop through each time step\n",
    "    for t in range(inputs.shape[1]):\n",
    "        X_t = inputs[:, t, :]  # (batch size, input dimension)\n",
    "        H = X_t @ W_xh + H @ W_hh + b_h  # update hidden state\n",
    "\n",
    "    return H  # final hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.286460Z",
     "start_time": "2021-12-10T04:02:06.279942Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f05f83c0debb279aa50701c729c102e5",
     "grade": true,
     "grade_id": "cell-5f600972f8df6f63",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "n, t, d, h = 2, 5, 8, 4 # batch size, sequence length, input dimension, hidden dimension\n",
    "\n",
    "inputs = torch.normal(0, 1, (n, t, d))\n",
    "initial_state  = torch.normal(0, 1, (n, h))\n",
    "\n",
    "W_xh = torch.normal(0, 1, (d, h))\n",
    "W_hh = torch.normal(0, 1, (h, h))\n",
    "b_h = torch.normal(0, 1, (1, h))\n",
    "assert torch.allclose(rnn(inputs, initial_state, W_xh, W_hh, b_h),\n",
    "                      torch.tensor([[ -46.5894,  -61.1859,   14.3644,   56.4997],\n",
    "                                    [ 166.5581,   84.0468,  -24.5756, -149.2971]]), rtol=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8699df7036671404a41bfb7a47d260b",
     "grade": false,
     "grade_id": "cell-9a87f61f0aa956c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Modern RNN Models\n",
    "\n",
    "We have introduced the basics of RNNs, which can better handle sequence data. However, such techniques may not be sufficient for practitioners when they face a wide range of sequence learning problems nowadays.\n",
    "\n",
    "For instance, a notable issue in practice is the numerical instability of RNNs. Although there are several implementation tricks such as gradient clipping, this issue can be alleviated further with more sophisticated designs of sequence models. Specifically, gated RNNs are much more common in practice. In this section, we will introduce you two of such widely-used networks, namely long short-term memory (LSTM) and gated recurrent units (GRUs).\n",
    "\n",
    "Here, we will only cover the basic concepts of GRU and LSTM. We won't discuss the architecture in details. If you are intereseted, you can refer to this [link](https://d2l.ai/chapter_recurrent-modern/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa2b1fefce743ade6fd9611aaf554eff",
     "grade": false,
     "grade_id": "cell-e719fae13d88cc4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Gated Recurrent Units (GRU)\n",
    "\n",
    "The key distinction between vanilla RNNs and GRUs is that the latter support gating of the hidden state. This means that we have dedicated mechanisms for when a hidden state should be updated and also when it should be reset. These mechanisms are learned and the network can selectively keep or forget information. For instance, if the first token is of great importance we will learn not to update the hidden state after the first observation. Likewise, we will learn to skip irrelevant temporary observations.\n",
    "\n",
    "<img src='img/gru.svg'>\n",
    "\n",
    "As shown in the figure above, the reset gates help capture short-term dependencies in sequences. And the update gates help capture long-term dependencies in sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d89e835d7f068564ef0af63df416a366",
     "grade": false,
     "grade_id": "cell-d5d014c4b87288c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To initialize a [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU) layer in PyTorch, try the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.294503Z",
     "start_time": "2021-12-10T04:02:06.287977Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b4f7f2a8ac03c4cb14a1390db9c1865",
     "grade": false,
     "grade_id": "cell-26ab854626e7bccb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([2, 5, 4])\n",
      "hn: torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "n, t, d, h = 2, 5, 8, 4 # batch size, sequence length, input dimension, hidden dimension\n",
    "\n",
    "# If batch_first=True, then the input and output tensors are \n",
    "# provided as (batch, seq, feature) instead of (seq, batch, feature).\n",
    "rnn = nn.GRU(input_size=d, hidden_size=h, batch_first=True)\n",
    "\n",
    "inputs = torch.randn(n, t, d)\n",
    "h0 = torch.randn(1, n, h) # the first dimension is the number of RNN layers (default value is 1)\n",
    "\n",
    "output, hn = rnn(inputs, h0)\n",
    "print('output:', output.shape)\n",
    "print('hn:', hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU Summary in PyTorch\n",
    "\n",
    "Inputs:\n",
    "\n",
    "inputs: (batch_size, seq_len, input_dim) ‚Üí (2, 5, 8)\n",
    "\n",
    "h0: (num_layers, batch_size, hidden_dim) ‚Üí (1, 2, 4)\n",
    "\n",
    "Outputs:\n",
    "\n",
    "output: hidden states for all time steps ‚Üí (2, 5, 4)\n",
    "\n",
    "hn: hidden state from the last time step ‚Üí (1, 2, 4)\n",
    "\n",
    "Intuition:\n",
    "\n",
    "output = sequence info (good for tasks needing predictions at every step).\n",
    "\n",
    "hn = summary of the sequence (good for tasks like classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f06eac258f0762775c5275b2d9f094da",
     "grade": false,
     "grade_id": "cell-fe2b5470e33ea8c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.2 Long Short-Term Memory (LSTM)\n",
    "\n",
    "The challenge to address long-term information preservation and short-term input skipping in latent variable models has existed for a long time. One of the earliest approaches to address this was the long short-term memory (LSTM) ([Hochreiter & Schmidhuber, 1997](https://direct.mit.edu/neco/article/9/8/1735/6109/Long-Short-Term-Memory)). It shares many of the properties of the GRU. Interestingly, LSTMs have a slightly more complex design than GRUs but predates GRUs by almost two decades.\n",
    "\n",
    "LSTM introduces a memory cell (or cell for short) that has the same shape as the hidden state, engineered to record additional information. To control the memory cell we need a number of gates. One gate is needed to read out the entries from the cell. We will refer to this as the output gate. A second gate is needed to decide when to read data into the cell. We refer to this as the input gate. Last, we need a mechanism to reset the content of the cell, governed by a forget gate. The motivation for such a design is the same as that of GRUs, namely to be able to decide when to remember and when to ignore inputs in the hidden state via a dedicated mechanism.\n",
    "\n",
    "<img src='img/lstm.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ab990bc6e894e913ec1cde85a04e45a",
     "grade": false,
     "grade_id": "cell-5f293c477444d133",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To initialize a [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) layer in PyTorch, try the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.302741Z",
     "start_time": "2021-12-10T04:02:06.296450Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "358cda51006ffe8ba39bbaa6b653dc22",
     "grade": false,
     "grade_id": "cell-632ffc2ed588593d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([2, 5, 4])\n",
      "hn: torch.Size([1, 2, 4])\n",
      "cn: torch.Size([1, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "n, t, d, h = 2, 5, 8, 4 # batch size, sequence length, input dimension, hidden dimension\n",
    "\n",
    "# If batch_first=True, then the input and output tensors are \n",
    "# provided as (batch, seq, feature) instead of (seq, batch, feature).\n",
    "rnn = nn.LSTM(input_size=d, hidden_size=h, batch_first=True)\n",
    "\n",
    "inputs = torch.randn(n, t, d)\n",
    "h0 = torch.randn(1, n, h) # the first dimension is the number of RNN layers (default value is 1)\n",
    "c0 = torch.randn(1, n, h)\n",
    "\n",
    "output, (hn, cn) = rnn(inputs, (h0, c0))\n",
    "print('output:', output.shape)\n",
    "print('hn:', hn.shape)\n",
    "print('cn:', cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern RNN Models\n",
    "Why do we need them?\n",
    "\n",
    "Vanilla RNNs suffer from:\n",
    "\n",
    "Numerical instability (gradients explode/vanish).\n",
    "\n",
    "Difficulty learning long-term dependencies.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Implementation tricks: gradient clipping, careful initialization.\n",
    "\n",
    "Architectural improvements: Gated RNNs (GRU & LSTM).\n",
    "\n",
    "üîπ 3.1 Gated Recurrent Units (GRU)\n",
    "Core Idea\n",
    "\n",
    "GRUs add gates to the RNN hidden state.\n",
    "\n",
    "Gates = learnable mechanisms that decide:\n",
    "\n",
    "When to update the hidden state.\n",
    "\n",
    "When to reset the hidden state.\n",
    "\n",
    "This allows GRUs to selectively keep or forget information.\n",
    "\n",
    "Intuition\n",
    "\n",
    "Update gate ‚Üí Should we carry forward old information or replace it with new input?\n",
    "\n",
    "Reset gate ‚Üí Should we ignore the past and reset memory for the next state?\n",
    "\n",
    "Example\n",
    "\n",
    "If the first token is very important (like the subject of a sentence), GRU can learn to not update the hidden state too much afterward.\n",
    "\n",
    "If the model encounters irrelevant tokens, it can reset and ignore them.\n",
    "\n",
    "‚úÖ Key takeaway:\n",
    "GRUs are more efficient than vanilla RNNs because they can adaptively remember or forget, making them good at handling longer sequences without blowing up or losing gradients.\n",
    "\n",
    "Modern RNN Models\n",
    "Why do we need them?\n",
    "\n",
    "Vanilla RNNs suffer from:\n",
    "\n",
    "Numerical instability (gradients explode/vanish).\n",
    "\n",
    "Difficulty learning long-term dependencies.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Implementation tricks: gradient clipping, careful initialization.\n",
    "\n",
    "Architectural improvements: Gated RNNs (GRU & LSTM).\n",
    "\n",
    "üîπ 3.1 Gated Recurrent Units (GRU)\n",
    "Core Idea\n",
    "\n",
    "GRUs add gates to the RNN hidden state.\n",
    "\n",
    "Gates = learnable mechanisms that decide:\n",
    "\n",
    "When to update the hidden state.\n",
    "\n",
    "When to reset the hidden state.\n",
    "\n",
    "This allows GRUs to selectively keep or forget information.\n",
    "\n",
    "Intuition\n",
    "\n",
    "Update gate ‚Üí Should we carry forward old information or replace it with new input?\n",
    "\n",
    "Reset gate ‚Üí Should we ignore the past and reset memory for the next state?\n",
    "\n",
    "Example\n",
    "\n",
    "If the first token is very important (like the subject of a sentence), GRU can learn to not update the hidden state too much afterward.\n",
    "\n",
    "If the model encounters irrelevant tokens, it can reset and ignore them.\n",
    "\n",
    "‚úÖ Key takeaway:\n",
    "GRUs are more efficient than vanilla RNNs because they can adaptively remember or forget, making them good at handling longer sequences without blowing up or losing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature     | GRU                    | LSTM                              |\n",
    "| ----------- | ---------------------- | --------------------------------- |\n",
    "| Gates       | 2 (update, reset)      | 3 (forget, input, output)         |\n",
    "| Memory Cell | No (hidden state only) | Yes (hidden + cell state)         |\n",
    "| Complexity  | Simpler, faster        | More complex, slower              |\n",
    "| Performance | Good for many tasks    | Better for long-term dependencies |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "477df5d8dc31519136543cbf66a17a5c",
     "grade": false,
     "grade_id": "cell-7ce478b19a724c2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment [60 points]\n",
    "\n",
    "In this assignment, you will use [MIMIC-III Demo](https://physionet.org/content/mimiciii-demo/) dataset, which contains all intensive care unit (ICU) stays for 100 patients. The task is Mortality Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6250f0bbc4007d8005a314910f560d53",
     "grade": false,
     "grade_id": "cell-0922073208c3d91c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Load Data\n",
    "\n",
    "In the previous lab, we have preprocessed the data. Thus, for this lab, we will directly use the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.434914Z",
     "start_time": "2021-12-10T04:02:06.307464Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "216b82ccd2e70cff91702086bc9ed1f9",
     "grade": false,
     "grade_id": "cell-db67447afa9a0fba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70e38c8aeb8bc703bb740c5713e7ae97",
     "grade": false,
     "grade_id": "cell-62c5dba8a899823d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here are the helper fuctions and CustomDataset from the previous lab. \n",
    "\n",
    "We will use the entire patient visit instead of only the last visit.\n",
    "\n",
    "Note that in this lab, we **do not** need to exclude patients with only one visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.441883Z",
     "start_time": "2021-12-10T04:02:06.437064Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d119834f8fa3599bc7b6baff31eb65a",
     "grade": false,
     "grade_id": "cell-9ec92c92777c8c9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# two helper functions\n",
    "\n",
    "TOTAL_NUM_CODES = 271\n",
    "\n",
    "\n",
    "def read_csv(filename):\n",
    "    \"\"\" reading csv from filename \"\"\"\n",
    "    data = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        csv_reader = csv.DictReader(file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "    header = list(data[0].keys())\n",
    "    return header, data\n",
    "\n",
    "\n",
    "def to_one_hot(label, num_class):\n",
    "    \"\"\" convert to one hot label \"\"\"\n",
    "    one_hot_label = [0] * num_class\n",
    "    for i in label:\n",
    "        one_hot_label[i] = 1\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read_csv(filename) ‚Üí Reads a CSV file and returns:\n",
    "\n",
    "header: list of column names.\n",
    "\n",
    "data: list of row dictionaries.\n",
    "\n",
    "to_one_hot(label, num_class) ‚Üí Converts label indices into a one-hot vector of length num_class.\n",
    "\n",
    "üëâ read_csv = data loader, to_one_hot = label encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.451168Z",
     "start_time": "2021-12-10T04:02:06.443700Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d40f5812beeadc025663e31978be2465",
     "grade": false,
     "grade_id": "cell-a186eb985edd96fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # read the csv\n",
    "        self._df = pd.read_csv(f'{DATA_PATH}/data.csv')\n",
    "        # split diagnosis code index by ';' and convert it to integer\n",
    "        self._df.icd9 = self._df.icd9.apply(lambda x: [int(i) for i in x.split(';')])\n",
    "        # build data dict\n",
    "        self._build_data_dict()\n",
    "        # a list of subject ids\n",
    "        self._subj_ids = list(self._data.keys())\n",
    "        # sort the subject ids to maintain a fixed order\n",
    "        self._subj_ids.sort()\n",
    "    \n",
    "    def _build_data_dict(self):\n",
    "        \"\"\" \n",
    "        build SUBJECT_ID to ADMISSION dict\n",
    "            - subject_id\n",
    "                - icd9: a list of ICD9 code index\n",
    "                - mortality: 0/1 morality label\n",
    "        \"\"\"\n",
    "        dict_data = {}\n",
    "        df = self._df.groupby('subject_id').agg({'mortality': lambda x: x.iloc[0], 'icd9': list}).reset_index()\n",
    "        for idx, row in df.iterrows():\n",
    "            subj_id = row.subject_id\n",
    "            dict_data[subj_id] = {}\n",
    "            dict_data[subj_id]['icd9'] = row.icd9\n",
    "            dict_data[subj_id]['mortality'] = row.mortality\n",
    "        self._data = dict_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\" return the number of samples (i.e. patients). \"\"\"\n",
    "        return len(self._subj_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" generates one sample of data. \"\"\"\n",
    "        # obtain the subject id\n",
    "        subj_id = self._subj_ids[index]\n",
    "        # obtain the data dict by subject id\n",
    "        data = self._data[subj_id]\n",
    "        # convert last admission's diagnosis code index to one hot\n",
    "        x = torch.tensor([to_one_hot(visit, TOTAL_NUM_CODES) for visit in data['icd9']], dtype=torch.float32)\n",
    "        # mortality label\n",
    "        y = torch.tensor(data['mortality'], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject_id,mortality,icd9\n",
    "1,0,12;45;78\n",
    "1,0,7;19\n",
    "2,1,4;5\n",
    "3,0,8\n",
    "Patient 1 has 2 admissions: [12,45,78] and [7,19]\n",
    "\n",
    "Patient 2 has 1 admission: [4,5]\n",
    "\n",
    "Patient 3 has 1 admission: [8]\n",
    "\n",
    "\n",
    "üîÑ How CustomDataset processes this\n",
    "\n",
    "Groups by subject_id.\n",
    "\n",
    "Keeps the first mortality label for each patient.\n",
    "\n",
    "Collects all admissions for that patient.\n",
    "\n",
    "So internally it builds:\n",
    "_data = {\n",
    "  1: { \"icd9\": [[12,45,78], [7,19]], \"mortality\": 0 },\n",
    "  2: { \"icd9\": [[4,5]], \"mortality\": 1 },\n",
    "  3: { \"icd9\": [[8]], \"mortality\": 0 }\n",
    "}\n",
    "üîë What happens in __getitem__(index)?\n",
    "\n",
    "Suppose TOTAL_NUM_CODES = 10 for simplicity.\n",
    "\n",
    "If we call dataset[0] ‚Üí Patient 1\n",
    "x = [\n",
    "  to_one_hot([12,45,78], 10),   # (these indices truncated in toy case)\n",
    "  to_one_hot([7,19], 10)\n",
    "]\n",
    "y = 0\n",
    "\n",
    "\n",
    "\n",
    "Output for dataset[0] (Patient 1)\n",
    "\n",
    "x (admissions, one-hot encoded):\n",
    "Output for dataset[0] (Patient 1)\n",
    "\n",
    "[[0,1,0,0,1,0,0,1,0,0],   # admission 1 ‚Üí codes 1,4,7\n",
    " [0,0,1,0,0,0,1,0,0,0]]   # admission 2 ‚Üí code 7, 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.473061Z",
     "start_time": "2021-12-10T04:02:06.452951Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed5d94e0397fab3b75e81d70c308de6d",
     "grade": false,
     "grade_id": "cell-fd6c0e9220bdbfe9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 99\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset()\n",
    "print('Size of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.478535Z",
     "start_time": "2021-12-10T04:02:06.474783Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c521f9d2e6348b1b13eb25449bf51c4f",
     "grade": false,
     "grade_id": "cell-bc87693d6901919d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 69\n",
      "Length of test dataset: 30\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "split = int(len(dataset)*0.7)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, test_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca01072280a0e9bb9a2e0cc50bbf4c1a",
     "grade": false,
     "grade_id": "cell-3d75144c592bf460",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is an example of $x$, and $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.486930Z",
     "start_time": "2021-12-10T04:02:06.480283Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15ffc3c9791dfdfd2aaa93e71b997316",
     "grade": false,
     "grade_id": "cell-bbf4c2c13369ead7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example x (shape torch.Size([1, 271])):\n",
      " tensor([[0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0.]])\n",
      "Example y:\n",
      " tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "print(f'Example x (shape {x.shape}):\\n', x)\n",
    "print(f'Example y:\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca435d10887461911c04925fbe653f0a",
     "grade": false,
     "grade_id": "cell-748697ceb2fe7d42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that $x$ is of shape $(2, 271)$, which means there are $271$ diagnosis codes in total, and this patient has two visits. It is in one-hot format. A $1$ in position $i$ means that diagnosis code of index $i$ appears in the that visit.\n",
    "\n",
    "And $y$ is either $0$ or $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce0efd0a8dc9c459cfeae1a266a14735",
     "grade": false,
     "grade_id": "cell-74d8c95ccf004960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Padding [20 points]\n",
    "\n",
    "In the previous lab, we implement a collate function `collate_fn()` to pad the sequence into the same length. For RNN, we will do something similar.\n",
    "\n",
    "Moreover, we will keep a separate variable storing the length of each sequence. Later, we will use this length variable to select the mask out padding visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.493336Z",
     "start_time": "2021-12-10T04:02:06.488360Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, total # diagnosis codes). Further, you need to store \n",
    "        the true length of each sequence into l.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patients, total # diagnosis codes, max # visits) of type torch.float\n",
    "        y: a tensor of shape (# patients) of type torch.float\n",
    "        l: a tensor of shape (# patients) of type torch.long\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of mortality labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [patient.shape[0] for patient in sequences]\n",
    "    total_num_codes = sequences[0].shape[1]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, total_num_codes), dtype=torch.float)\n",
    "    l = None\n",
    "    \n",
    "    # your code here\n",
    "    # Pad each patient's sequence to max_num_visits\n",
    "    x = torch.zeros((num_patients, max_num_visits, total_num_codes), dtype=torch.float)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        n_vis = seq.shape[0]\n",
    "        x[i, :n_vis, :] = torch.tensor(seq, dtype=torch.float)\n",
    "\n",
    "    # Lengths as tensor (number of visits per patient)\n",
    "    l = torch.tensor(num_visits, dtype=torch.long)\n",
    "    \n",
    "    return x, y, l\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a collate_fn when your data samples are not simple, fixed-size tensors, or when you want custom logic for batching. For your current data (single fixed-size vector and label), the default is probably fine‚Äîbut if you change your data structure, you'll need to write your own collate_fn to tell the DataLoader how to assemble a batch from individual items.\n",
    "\n",
    "\n",
    "\n",
    "Example:\n",
    "# Sample data: (sequence, label) pairs\n",
    "# Patient 1: 2 visits, 5 codes per visit\n",
    "seq1 = np.array([\n",
    "    [1, 0, 0, 0, 1],  # visit 1\n",
    "    [0, 1, 1, 0, 0],  # visit 2\n",
    "])\n",
    "label1 = 1\n",
    "\n",
    "# Patient 2: 3 visits\n",
    "seq2 = np.array([\n",
    "    [0, 1, 0, 1, 0],  # visit 1\n",
    "    [1, 0, 0, 0, 1],  # visit 2\n",
    "    [0, 0, 1, 1, 0],  # visit 3\n",
    "])\n",
    "label2 = 0\n",
    "\n",
    "# Patient 3: 1 visit\n",
    "seq3 = np.array([\n",
    "    [0, 0, 0, 1, 1],  # visit 1\n",
    "])\n",
    "label3 = 1\n",
    "\n",
    "data = [\n",
    "    (seq1, label1),\n",
    "    (seq2, label2),\n",
    "    (seq3, label3),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "Output of coallet func\n",
    "Output explained\n",
    "\n",
    "    x: shape (3, 3, 5) ‚Äî 3 patients, padded to 3 visits, 5 codes per visit.\n",
    "    y: tensor of shape (3,) ‚Äî [1., 0., 1.]\n",
    "    l: tensor of shape (3,) ‚Äî [2, 3, 1] (number of visits per patient)\n",
    "\n",
    "Printed output:\n",
    "Code\n",
    "\n",
    "x shape: torch.Size([3, 3, 5])\n",
    "tensor([\n",
    "  [[1., 0., 0., 0., 1.],\n",
    "   [0., 1., 1., 0., 0.],\n",
    "   [0., 0., 0., 0., 0.]],    # Patient 1 (padded)\n",
    "  [[0., 1., 0., 1., 0.],\n",
    "   [1., 0., 0., 0., 1.],\n",
    "   [0., 0., 1., 1., 0.]],    # Patient 2 (no pad)\n",
    "  [[0., 0., 0., 1., 1.],\n",
    "   [0., 0., 0., 0., 0.],\n",
    "   [0., 0., 0., 0., 0.]]     # Patient 3 (padded)\n",
    "])\n",
    "y: tensor([1., 0., 1.])\n",
    "l: tensor([2, 3, 1])\n",
    "\n",
    "Loop explanation:\n",
    "\n",
    "Example\n",
    "\n",
    "Suppose:\n",
    "\n",
    "    max_num_visits = 5\n",
    "    total_num_codes = 3\n",
    "\n",
    "Patient 1 has 2 visits:\n",
    "Python\n",
    "\n",
    "seq = [[1,0,0],[0,1,0]]\n",
    "n_vis = 2\n",
    "x[0, :2, :] = [[1,0,0],[0,1,0]]\n",
    "# x[0, 2:, :] = [[0,0,0],[0,0,0],[0,0,0]]  # padding\n",
    "\n",
    "Patient 2 has 5 visits:\n",
    "Python\n",
    "\n",
    "seq = [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,0,1]]\n",
    "n_vis = 5\n",
    "x[1, :5, :] = seq\n",
    "# no padding necessary\n",
    "\n",
    "Summary\n",
    "\n",
    "    This loop populates the batch tensor x with each patient‚Äôs visit data, left-aligning it and padding with zeros so all patients have the same number of visits (max in the batch).\n",
    "    This is essential for batching variable-length sequences for models like RNNs or transformers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.506392Z",
     "start_time": "2021-12-10T04:02:06.494976Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88cf22df0c8affdcef7954649474da40",
     "grade": true,
     "grade_id": "cell-93a8f004ead104ab",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 271])\n",
      "torch.Size([4])\n",
      "tensor([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=4, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, y, l = next(loader_iter)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(l)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert l.dtype == torch.long\n",
    "\n",
    "assert x.shape[0] == 4\n",
    "assert x.shape[-1] == 271\n",
    "assert y.shape == (4,)\n",
    "assert l.shape == (4,)\n",
    "\n",
    "for i in range(4):\n",
    "    real_x, real_y = train_dataset[i]\n",
    "    assert len(real_x) == l[i]\n",
    "    for j in range(real_x.shape[0]):\n",
    "        visit = real_x[j]\n",
    "        got = x[i, j, :]\n",
    "        assert all(visit == got)\n",
    "        assert real_y == y[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d27f6c9c098c23dbc396947903c99f8b",
     "grade": false,
     "grade_id": "cell-2e7975367aeb8d4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data Loader\n",
    "\n",
    "Now, we can load the dataset into the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.511775Z",
     "start_time": "2021-12-10T04:02:06.508001Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68bfd6d30a647aa8dca5696f1f21dd72",
     "grade": false,
     "grade_id": "cell-969cadf4d62ef5d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train batches: 18\n",
      "# of test batches: 8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# how many samples per batch to load\n",
    "batch_size = 4\n",
    "\n",
    "# prepare dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "print(\"# of train batches:\", len(train_loader))\n",
    "print(\"# of test batches:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.518113Z",
     "start_time": "2021-12-10T04:02:06.513595Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2528e316246f838836b129a33d931a46",
     "grade": false,
     "grade_id": "cell-e34dc46c974a1ea7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a batch x: torch.Size([4, 1, 271])\n",
      "Shape of a batch y: torch.Size([4])\n",
      "Shape of a batch l: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(train_loader)\n",
    "x, y, l = next(train_iter)\n",
    "\n",
    "print('Shape of a batch x:', x.shape)\n",
    "print('Shape of a batch y:', y.shape)\n",
    "print('Shape of a batch l:', l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test ensures your batch loader:\n",
    "\n",
    "    Pads visit sequences correctly.\n",
    "    Tracks the correct number of visits.\n",
    "    Produces the right types/shapes for modeling.\n",
    "    Preserves the original data and labels for each batch element.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26b1372c8d9f9d60271b69bd658e9ea9",
     "grade": false,
     "grade_id": "cell-369f8724259879e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Build the Model\n",
    "\n",
    "<img src='img/naive-rnn.png'>\n",
    "\n",
    "We will construct this simple RNN structure. So each input is a one-hot vector. At the 0-th visit, this has $\\boldsymbol{X}_0$, and at t-th visit, this has $\\boldsymbol{X}_t$.\n",
    "\n",
    "Each one of them will then map to a hidden state $\\boldsymbol{h}_t$. The hidden state $\\boldsymbol{h}_t$ can be determined by $\\boldsymbol{h}_{t-1}$ and the corresponding current input $\\boldsymbol{X}_t$.\n",
    "\n",
    "Finally, once we have the $\\boldsymbol{h}_T$, the hidden state of the last timestamp, then we can use this as feature vectors and train a NN to perform the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b90084c0ccec7e3fd14c847f07363350",
     "grade": false,
     "grade_id": "cell-23394ecb0a77ee62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let us build this model. The forward steps will be:\n",
    "\n",
    "    1. Pass the inputs through the RNN layer;\n",
    "    2. Obtain the hidden state at the last visit;\n",
    "    3. Pass the hidden state through the linear and activation layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13ee714d778005ee06addc89098e53fd",
     "grade": false,
     "grade_id": "cell-92ef22ca87069002",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Mask Selection [20 points]\n",
    "\n",
    "Importantly, you need to use `length` to mask out the paddings in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.522548Z",
     "start_time": "2021-12-10T04:02:06.519611Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, length):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, hidden_dim)\n",
    "        length: the true visit length of shape (batch_size,)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, hidden_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    idx = (length - 1).unsqueeze(1).unsqueeze(2)    # shape: (batch, 1, 1)\n",
    "    idx = idx.expand(-1, 1, hidden_states.size(2))  # shape: (batch, 1, hidden_dim)\n",
    "    last_hidden_state = hidden_states.gather(1, idx).squeeze(1)  # shape: (batch, hidden_dim)\n",
    "    return last_hidden_state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "hidden_states = torch.tensor([\n",
    "    [[10, 11], [12, 13], [14, 15], [16, 17]],\n",
    "    [[20, 21], [22, 23], [24, 25], [26, 27]],\n",
    "    [[30, 31], [32, 33], [34, 35], [36, 37]],\n",
    "])\n",
    "length = torch.tensor([2, 4, 3])\n",
    "\n",
    "idx = (length - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, hidden_states.size(2))\n",
    "print(\"Index tensor for gather:\\n\", idx)\n",
    "\n",
    "selected = hidden_states.gather(1, idx)\n",
    "print(\"Gathered hidden states (with singleton dimension):\\n\", selected)\n",
    "\n",
    "last_hidden_state = selected.squeeze(1)\n",
    "print(\"Final last_hidden_state tensor:\\n\", last_hidden_state)\n",
    "\n",
    "\n",
    "\n",
    "Each step  calculation\n",
    "#idx\n",
    "Let‚Äôs use your specific example:  \n",
    "You have a tensor: `length - 1 = tensor([1, 3, 2])` (for 3 patients).\n",
    "\n",
    "Here‚Äôs how `.unsqueeze(1)`, `.unsqueeze(2)`, and `.expand(-1, 1, hidden_dim)` work:\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step with shapes and values\n",
    "\n",
    "### 1. Start with\n",
    "```python\n",
    "idx = torch.tensor([1, 3, 2])           # Shape: (3,)\n",
    "```\n",
    "#### Values:\n",
    "```\n",
    "[1, 3, 2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `.unsqueeze(1)`  \n",
    "Adds a dimension at position 1:\n",
    "```python\n",
    "idx = idx.unsqueeze(1)                  # Shape: (3, 1)\n",
    "```\n",
    "#### Values:\n",
    "```\n",
    "[[1],\n",
    " [3],\n",
    " [2]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `.unsqueeze(2)`  \n",
    "Adds another dimension at position 2:\n",
    "```python\n",
    "idx = idx.unsqueeze(2)                  # Shape: (3, 1, 1)\n",
    "```\n",
    "#### Values:\n",
    "```\n",
    "[[[1]],\n",
    " [[3]],\n",
    " [[2]]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `.expand(-1, 1, hidden_dim)`  \n",
    "Suppose `hidden_dim = 2` (it could be any number, but let's use 2 for clarity).\n",
    "\n",
    "- `.expand(-1, 1, 2)` means:\n",
    "  - Keep the same size in the first dimension (`3` patients)\n",
    "  - Keep the same size in the second dimension (`1`)\n",
    "  - Expand the third dimension to `2` (hidden_dim)\n",
    "\n",
    "```python\n",
    "idx = idx.expand(-1, 1, 2)              # Shape: (3, 1, 2)\n",
    "```\n",
    "#### Values:\n",
    "```\n",
    "[[[1, 1]],\n",
    " [[3, 3]],\n",
    " [[2, 2]]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **What does this mean?**\n",
    "\n",
    "- Each patient now has a row of the form `[[index, index]]`, where `index` is the visit you want for that patient, and it‚Äôs repeated for each hidden dimension.\n",
    "- This shape and value matches what PyTorch‚Äôs `.gather()` expects:  \n",
    "  You want to select, for each patient, the row (visit) at their specific index, and all hidden_dim values for that row.\n",
    "\n",
    "---\n",
    "\n",
    "## **In summary:**\n",
    "- `.unsqueeze(1)` changes shape from `(3,)` to `(3,1)`\n",
    "- `.unsqueeze(2)` changes shape from `(3,1)` to `(3,1,1)`\n",
    "- `.expand(-1,1,2)` changes shape from `(3,1,1)` to `(3,1,2)` and repeats each index for the hidden dimension.\n",
    "\n",
    "**This prepares your index tensor so you can batch-select each patient‚Äôs last (non-padding) hidden state across all hidden units at once.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Visual Table**\n",
    "\n",
    "| Patient | index | after expand (hidden_dim=2) |\n",
    "|---------|-------|-----------------------------|\n",
    "|    0    |   1   | [[1, 1]]                   |\n",
    "|    1    |   3   | [[3, 3]]                   |\n",
    "|    2    |   2   | [[2, 2]]                   |\n",
    "\n",
    "---\n",
    "#selected\n",
    "Step 1: Gather\n",
    "What does hidden_states.gather(1, idx) do?\n",
    "\n",
    "    hidden_states shape: (batch=3, visits=4, hidden_dim=2)\n",
    "    idx shape: (3, 1, 2)\n",
    "\n",
    "How gather works:\n",
    "For each patient (batch), gather takes the row (visit) along dimension 1 (the visit axis) at the index given in idx, for all hidden_dim.\n",
    "Calculation for each patient\n",
    "Patient 1 (batch 0):\n",
    "\n",
    "    idx[0] = [[1, 1]]: means visit index 1, for both hidden dims.\n",
    "    hidden_states[0, 1, :] = [12, 13]\n",
    "\n",
    "Patient 2 (batch 1):\n",
    "\n",
    "    idx[1] = [[3, 3]]: means visit index 3, for both hidden dims.\n",
    "    hidden_states[1, 3, :] = [26, 27]\n",
    "\n",
    "Patient 3 (batch 2):\n",
    "\n",
    "    idx[2] = [[2, 2]]: means visit index 2, for both hidden dims.\n",
    "    hidden_states[2, 2, :] = [34, 35]\n",
    "\n",
    "So, after gather:\n",
    "Code\n",
    "\n",
    "selected = tensor([\n",
    "    [[12, 13]],   # Patient 1\n",
    "    [[26, 27]],   # Patient 2\n",
    "    [[34, 35]],   # Patient 3\n",
    "])  # shape: (3, 1, 2)\n",
    "\n",
    "Step 2: Squeeze\n",
    "\n",
    "    selected.squeeze(1) removes the singleton dimension (of size 1) at position 1.\n",
    "\n",
    "Result:\n",
    "Code\n",
    "\n",
    "last_hidden_state = tensor([\n",
    "    [12, 13],\n",
    "    [26, 27],\n",
    "    [34, 35],\n",
    "])  # shape: (3, 2)\n",
    "\n",
    "Summary Table\n",
    "Patient\tidx\tGather result\tAfter squeeze (final)\n",
    "0\t[1, 1]\t[[12, 13]]\t[12, 13]\n",
    "1\t[3, 3]\t[[26, 27]]\t[26, 27]\n",
    "2\t[2, 2]\t[[34, 35]]\t[34, 35]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.529890Z",
     "start_time": "2021-12-10T04:02:06.524301Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83d69fc427060418c0b9399251dc4e8b",
     "grade": true,
     "grade_id": "cell-26114cc62b765649",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "max_num_visits = 10\n",
    "batch_size = 16\n",
    "hidden_dim = 100\n",
    "\n",
    "hidden_states = torch.randn((batch_size, max_num_visits, hidden_dim))\n",
    "lengths = torch.tensor([random.randint(1, max_num_visits) for _ in range(batch_size)])\n",
    "out = get_last_visit(hidden_states, lengths)\n",
    "\n",
    "assert out.shape == (batch_size, hidden_dim)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    last_h = 0\n",
    "    last_h = hidden_states[b, lengths[b] - 1]\n",
    "    assert torch.allclose(out[b], last_h, atol=1e-4), \\\n",
    "    \"The last visit's hidden state of %d-th visit of the %d-th patient is wrong. \"%(v,b) +\\\n",
    "    \"Expect {} Got {} with your get_last_visit\".format(last_h, out[b])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "556e1fb024ea6f911e687f83603cd129",
     "grade": false,
     "grade_id": "cell-1ec1d62fd10f3408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Build NaiveRNN [20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.544515Z",
     "start_time": "2021-12-10T04:02:06.531252Z"
    },
    "deletable": false
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83/3605706337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# load the model here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_83/3605706337.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_last_visit(hidden_states, length):\n",
    "    idx = (length - 1).unsqueeze(1).unsqueeze(2).expand(-1, 1, hidden_states.size(2))\n",
    "    last_hidden_state = hidden_states.gather(1, idx).squeeze(1)\n",
    "    return last_hidden_state\n",
    "\n",
    "class NaiveRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Naive RNN model using GRU, Linear, and Sigmoid layers.\n",
    "    \"\"\"\n",
    "    TOTAL_NUM_CODES = 271 \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1. Define the RNN using nn.GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.TOTAL_NUM_CODES,\n",
    "            hidden_size=32,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # 2. Define the linear layer\n",
    "        self.linear = nn.Linear(32, 1)\n",
    "        # 3. Define the final activation layer\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, length):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            length: the true visit length of shape (batch_size,)\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        # 1. Pass the inputs through the RNN layer\n",
    "        out, _ = self.rnn(x)\n",
    "        # 2. Obtain the hidden state at the last visit using get_last_visit()\n",
    "        last_hidden = get_last_visit(out, length)\n",
    "        # 3. Pass the hidden state through the linear and activation layers\n",
    "        logits = self.linear(last_hidden).squeeze(-1)  # shape: (batch_size)\n",
    "        probs = self.act(logits)\n",
    "        return probs\n",
    "\n",
    "# load the model here\n",
    "model = NaiveRNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.553716Z",
     "start_time": "2021-12-10T04:02:06.546876Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e7fedfad9f4ff312c101f1b134cd44c",
     "grade": true,
     "grade_id": "cell-f22f078c9d3401a0",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "layers_to_check = [nn.GRU, nn.Linear, nn.Sigmoid]\n",
    "for layer_to_check in layers_to_check:\n",
    "    no_layer = True\n",
    "    for child in model.children():\n",
    "        for layer in child.modules():\n",
    "            if(isinstance(layer, layer_to_check)):\n",
    "                no_layer = False\n",
    "    assert not no_layer, \"{} is missing in your RNN\".format(layer_to_check)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, y, l = next(loader_iter)\n",
    "model_output = model(x, l)\n",
    "assert model_output.shape == (10,), \"Your RNN's output shape is {}, expect {}\".format(model_output.shape, (10,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09f53939afb0c1281cdf4c119622b14e",
     "grade": false,
     "grade_id": "cell-69d86cdb0ee89e46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train the Network\n",
    "\n",
    "In this step, you will train the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:06.557956Z",
     "start_time": "2021-12-10T04:02:06.555274Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22f95d571e3dc179ead5e9369b5eb2ff",
     "grade": false,
     "grade_id": "cell-e048585cb94b4435",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use Binary Cross Entropy as the loss function (`nn.BCELoss`)\n",
    "# Use Adam as the optimizer (`torch.optim.Adam`)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-17T10:41:10.932858Z",
     "start_time": "2021-06-17T10:41:10.924864Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d40074d948a21387ecb1996396fb380",
     "grade": false,
     "grade_id": "cell-152e9aafcaa7a0ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:07.085255Z",
     "start_time": "2021-12-10T04:02:06.559522Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3aa35a55451633aa5464c21baffffc21",
     "grade": false,
     "grade_id": "cell-3b8eb35770a1bf5b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "#input: Y_score,Y_pred,Y_true\n",
    "#output: accuracy, auc, precision, recall, f1-score\n",
    "def classification_metrics(Y_score, Y_pred, Y_true):\n",
    "    acc, auc, precision, recall, f1score = accuracy_score(Y_true, Y_pred), \\\n",
    "                                           roc_auc_score(Y_true, Y_score), \\\n",
    "                                           precision_score(Y_true, Y_pred), \\\n",
    "                                           recall_score(Y_true, Y_pred), \\\n",
    "                                           f1_score(Y_true, Y_pred)\n",
    "    return acc, auc, precision, recall, f1score\n",
    "\n",
    "\n",
    "#input: model, loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_y_true = torch.LongTensor()\n",
    "    all_y_pred = torch.LongTensor()\n",
    "    all_y_score = torch.FloatTensor()\n",
    "    for x, y, l in loader:\n",
    "        # pass the input through the model\n",
    "        y_hat = model(x, l)\n",
    "        # convert shape from [batch size, 1] to [batch size]\n",
    "        y_hat = y_hat.view(y_hat.shape[0])\n",
    "        y_pred = (y_hat > 0.5).type(torch.float)\n",
    "        all_y_true = torch.cat((all_y_true, y.to('cpu')), dim=0)\n",
    "        all_y_pred = torch.cat((all_y_pred,  y_pred.to('cpu')), dim=0)\n",
    "        all_y_score = torch.cat((all_y_score,  y_hat.to('cpu')), dim=0)\n",
    "        \n",
    "    acc, auc, precision, recall, f1 = classification_metrics(all_y_score.detach().numpy(), \n",
    "                                                             all_y_pred.detach().numpy(), \n",
    "                                                             all_y_true.detach().numpy())\n",
    "    print(f\"acc: {acc:.3f}, auc: {auc:.3f}, precision: {precision:.3f}, recall: {recall:.3f}, f1: {f1:.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:07.119395Z",
     "start_time": "2021-12-10T04:02:07.086650Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "953bcca02953fe4bb35432d9cdc88a57",
     "grade": false,
     "grade_id": "cell-129173e0dec74bf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"model perfomance before training:\")\n",
    "evaluate(model, train_loader)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T04:02:07.912476Z",
     "start_time": "2021-12-10T04:02:07.121481Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "667864a66702032edc23b209a217f554",
     "grade": false,
     "grade_id": "cell-b23c9926657ef7f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "# feel free to change this\n",
    "n_epochs = 15\n",
    "\n",
    "# prep model for training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    for x, y, l in train_loader:\n",
    "        \"\"\" Step 1. clear gradients \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\"  Step 2. perform forward pass using `model`, save the output to y_hat \"\"\"\n",
    "        y_hat = model(x, l)\n",
    "        \"\"\" Step 3. calculate the loss using `criterion`, save the output to loss. \"\"\"\n",
    "        # convert shape from [batch size, 1] to [batch size]\n",
    "        y_hat = y_hat.view(y_hat.shape[0])\n",
    "        loss = criterion(y_hat, y)\n",
    "        \"\"\" Step 4. backward pass \"\"\"\n",
    "        loss.backward()\n",
    "        \"\"\" Step 5. optimization \"\"\"\n",
    "        optimizer.step()\n",
    "        \"\"\" Step 6. record loss \"\"\"\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    evaluate(model, train_loader)\n",
    "    evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ebe391699848bbb41c374f47fae453e",
     "grade": false,
     "grade_id": "cell-25544e26b197fe35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The result is bad due to very limited data. The model overfits the training data very fast.\n",
    "\n",
    "You are encouraged to try this on the whole MIMIC-III dataset. The result will be much more promising!"
   ]
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/LAB4/LAB4.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
